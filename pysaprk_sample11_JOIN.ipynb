{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e585834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/13 00:24:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").\\\n",
    "                                     appName(\"spark_on_docker\").\\\n",
    "                                     getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b7d504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/retail-data/all/*.csv\")\\\n",
    "  .coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbed394b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9e553a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,StringType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,IntegerType,true),StructField(Country,StringType,true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aedbbb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dfTable\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc179fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af07474f",
   "metadata": {},
   "source": [
    "Aggregation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe960b",
   "metadata": {},
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e235cae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(InvoiceNo)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"InvoiceNo\")).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6337ba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------------+\n",
      "|count(1)|count(1)|count(InvoiceNo)|\n",
      "+--------+--------+----------------+\n",
      "|  541909|  541909|          541909|\n",
      "+--------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*), count(1), count('InvoiceNo') from dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef52b71",
   "metadata": {},
   "source": [
    "countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adb3a4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.select(countDistinct(\"StockCode\")).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ffb020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(DISTINCT StockCode) from dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c12c8",
   "metadata": {},
   "source": [
    "approx_count_distinct\n",
    "\n",
    "Often, we find ourselves working with large datasets and the exact distinct count is irrelevant.\n",
    "There are times when an approximation to a certain degree of accuracy will work just fine, and\n",
    "for that, you can use the approx_count_distinct function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "773a0930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/13 00:25:14 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 25:>                                                         (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   1|   2|\n",
      "+----+----+\n",
      "|3804|3364|\n",
      "+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(\\\n",
    "    approx_count_distinct(\"StockCode\").alias('1'), \\\n",
    "    approx_count_distinct(\"StockCode\", 0.1).alias('2')).show() # 3364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d675c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select approx_count_distinct(StockCode, 0.1) from dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da7efb",
   "metadata": {},
   "source": [
    "first and last\n",
    "You can get the first and last values from a DataFrame by using these two obviously named\n",
    "functions. This will be based on the rows in the DataFrame, not on the values in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3988a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a1ac46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT first(StockCode), last(StockCode) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327d02c",
   "metadata": {},
   "source": [
    "min and max\n",
    "To extract the minimum and maximum values from a DataFrame, use the min and max functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d543f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86896c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT min(Quantity), max(Quantity) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef03154",
   "metadata": {},
   "source": [
    "sum\n",
    "Another simple task is to add all the values in a row using the sum function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d7dc210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show() # 5176450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee726e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT sum(Quantity) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bb4f7a",
   "metadata": {},
   "source": [
    "sumDistinct\n",
    "\n",
    "In addition to summing a total, you also can sum a distinct set of values by using the\n",
    "sumDistinct function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e48f9832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum_distinct\n",
    "df.select(sum_distinct(\"Quantity\")).show() # 29310"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435cda93",
   "metadata": {},
   "source": [
    "avg\n",
    "\n",
    "Although you can calculate average by dividing sum by count, Spark provides an easier way to\n",
    "get that value via the avg or mean functions. In this example, we use alias in order to more\n",
    "easily reuse these columns later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b5cf4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    ".selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42a72828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------------+----------------+\n",
      "|transactions quanty rate|   avg_purchases|  mean_purchases|\n",
      "+------------------------+----------------+----------------+\n",
      "|        9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    (sum(\"Quantity\")/count(\"Quantity\")).alias(\"transactions quanty rate\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d57f6",
   "metadata": {},
   "source": [
    "Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0083d414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n",
      "| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "|47559.303646609056|47559.391409298754|  218.08095663447796|   218.08115785023418|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e3ea3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n",
      "| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "|47559.303646609056|47559.391409298754|  218.08095663447796|   218.08115785023418|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT var_pop(Quantity), \\\n",
    "    var_samp(Quantity),stddev_pop(Quantity), \\\n",
    "    stddev_samp(Quantity) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d9d7d",
   "metadata": {},
   "source": [
    "skewness and kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae7d2786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052562|119768.05495536952|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46ec9d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052562|119768.05495536952|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT skewness(Quantity), kurtosis(Quantity) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c11a4",
   "metadata": {},
   "source": [
    "Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35bd861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:>                                                         (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085635685E-4|             1052.7280543902734|            1052.7260778741693|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "df.select(\\\n",
    "    corr(\"InvoiceNo\", \"Quantity\"), \\\n",
    "    covar_samp(\"InvoiceNo\", \"Quantity\"),\\\n",
    "    covar_pop(\"InvoiceNo\", \"Quantity\")\\\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87e12b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:==================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085635685E-4|             1052.7280543902734|            1052.7260778741693|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "r = spark.sql(\"SELECT corr(InvoiceNo, Quantity), \\\n",
    "                  covar_samp(InvoiceNo, Quantity),\\\n",
    "                  covar_pop(InvoiceNo, Quantity)\\\n",
    "            FROM dfTable\")\n",
    "r.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a3ab0",
   "metadata": {},
   "source": [
    "Aggregating to Complex Types\n",
    "\n",
    "In Spark, you can perform aggregations not just of numerical values using formulas, you can also\n",
    "perform them on complex types.For example, we can collect a list of values present in a given\n",
    "column or only the unique values by collecting to a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "136762b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                   A|                   B|\n",
      "+--------------------+--------------------+\n",
      "|[Portugal, Italy,...|[United Kingdom, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "df.agg(collect_set(\"Country\").alias(\"A\"), collect_list(\"Country\").alias(\"B\")).selectExpr(\"A\", \"B\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a319ecb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|collect_set(Country)|collect_set(Country)|\n",
      "+--------------------+--------------------+\n",
      "|[Portugal, Italy,...|[Portugal, Italy,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT collect_set(Country), collect_set(Country) FROM dfTable\").show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16465b55",
   "metadata": {},
   "source": [
    "Grouping\n",
    "\n",
    "Thus far, we have performed only DataFrame-level aggregations. A more common task is to\n",
    "perform calculations based on groups in the data. This is typically done on categorical data for\n",
    "which we group our data on one column and perform some calculations on the other columns\n",
    "that end up in that group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1ea6ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:>                                                         (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   537159|     14527|   28|\n",
      "|   537213|     12748|    6|\n",
      "|   538191|     15061|   16|\n",
      "|  C539301|     13496|    1|\n",
      "|   539391|     12417|   11|\n",
      "|   539462|     14032|   15|\n",
      "|  C539710|     14769|    2|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38b2a11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:==================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n",
      "|InvoiceNo|CustomerId|count(1)|\n",
      "+---------+----------+--------+\n",
      "|   536846|     14573|      76|\n",
      "|   537026|     12395|      12|\n",
      "|   537883|     14437|       5|\n",
      "|   538068|     17978|      12|\n",
      "|   538279|     14952|       7|\n",
      "|   538800|     16458|      10|\n",
      "|   538942|     17346|      12|\n",
      "|  C539947|     13854|       1|\n",
      "|   540096|     13253|      16|\n",
      "|   540530|     14755|      27|\n",
      "|   541225|     14099|      19|\n",
      "|   541978|     13551|       4|\n",
      "|   542093|     17677|      16|\n",
      "|   537159|     14527|      28|\n",
      "|   537213|     12748|       6|\n",
      "|   538191|     15061|      16|\n",
      "|  C539301|     13496|       1|\n",
      "|   539391|     12417|      11|\n",
      "|   539462|     14032|      15|\n",
      "|  C539710|     14769|       2|\n",
      "+---------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT InvoiceNo, CustomerId, count(*) FROM dfTable GROUP BY InvoiceNo, CustomerId\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a339c5",
   "metadata": {},
   "source": [
    "Grouping with Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6939660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536938|  14|             14|\n",
      "|   537252|   1|              1|\n",
      "|   537691|  20|             20|\n",
      "|   538041|   1|              1|\n",
      "|   538184|  26|             26|\n",
      "|   538517|  53|             53|\n",
      "|   538879|  19|             19|\n",
      "|   539275|   6|              6|\n",
      "|   539630|  12|             12|\n",
      "|   540499|  24|             24|\n",
      "|   540540|  22|             22|\n",
      "|  C540850|   1|              1|\n",
      "|   540976|  48|             48|\n",
      "|   541432|   4|              4|\n",
      "|   541518| 101|            101|\n",
      "|   541783|  35|             35|\n",
      "|   542026|   9|              9|\n",
      "|   542375|   6|              6|\n",
      "|   536597|  28|             28|\n",
      "+---------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03deb4b2",
   "metadata": {},
   "source": [
    "Grouping with Expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1c319b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 94:=======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----+---------------+\n",
      "|InvoiceNo|CustomerId|quan|count(Quantity)|\n",
      "+---------+----------+----+---------------+\n",
      "|   536846|     14573|  76|             76|\n",
      "|   537026|     12395|  12|             12|\n",
      "|   537883|     14437|   5|              5|\n",
      "|   538068|     17978|  12|             12|\n",
      "|   538279|     14952|   7|              7|\n",
      "|   538800|     16458|  10|             10|\n",
      "|   538942|     17346|  12|             12|\n",
      "|  C539947|     13854|   1|              1|\n",
      "|   540096|     13253|  16|             16|\n",
      "|   540530|     14755|  27|             27|\n",
      "|   541225|     14099|  19|             19|\n",
      "|   541978|     13551|   4|              4|\n",
      "|   542093|     17677|  16|             16|\n",
      "|   537159|     14527|  28|             28|\n",
      "|   537213|     12748|   6|              6|\n",
      "|   538191|     15061|  16|             16|\n",
      "|  C539301|     13496|   1|              1|\n",
      "|   539391|     12417|  11|             11|\n",
      "|   539462|     14032|  15|             15|\n",
      "|  C539710|     14769|   2|              2|\n",
      "+---------+----------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76a310",
   "metadata": {},
   "source": [
    "Grouping with Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "496cd53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:=======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "|   538184|12.076923076923077|   8.142590198943392|\n",
      "|   538517|3.0377358490566038|  2.3946659604837897|\n",
      "|   538879|21.157894736842106|  11.811070444356483|\n",
      "|   539275|              26.0|  12.806248474865697|\n",
      "|   539630|20.333333333333332|  10.225241100118645|\n",
      "|   540499|              3.75|  2.6653642652865788|\n",
      "|   540540|2.1363636363636362|  1.0572457590557278|\n",
      "|  C540850|              -1.0|                 0.0|\n",
      "|   540976|10.520833333333334|   6.496760677872902|\n",
      "|   541432|             12.25|  10.825317547305483|\n",
      "|   541518| 23.10891089108911|  20.550782784878713|\n",
      "|   541783|11.314285714285715|   8.467657556242811|\n",
      "|   542026| 7.666666666666667|   4.853406592853679|\n",
      "|   542375|               8.0|  3.4641016151377544|\n",
      "|   536597|2.5357142857142856|  2.7448932175059566|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff934508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "|   538184|12.076923076923077|   8.142590198943392|\n",
      "|   538517|3.0377358490566038|  2.3946659604837897|\n",
      "|   538879|21.157894736842106|  11.811070444356483|\n",
      "|   539275|              26.0|  12.806248474865697|\n",
      "|   539630|20.333333333333332|  10.225241100118645|\n",
      "|   540499|              3.75|  2.6653642652865788|\n",
      "|   540540|2.1363636363636362|  1.0572457590557278|\n",
      "|  C540850|              -1.0|                 0.0|\n",
      "|   540976|10.520833333333334|   6.496760677872902|\n",
      "|   541432|             12.25|  10.825317547305483|\n",
      "|   541518| 23.10891089108911|  20.550782784878713|\n",
      "|   541783|11.314285714285715|   8.467657556242811|\n",
      "|   542026| 7.666666666666667|   4.853406592853679|\n",
      "|   542375|               8.0|  3.4641016151377544|\n",
      "|   536597|2.5357142857142856|  2.7448932175059566|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT InvoiceNo, avg(Quantity), stddev_pop(Quantity) FROM dfTable GROUP BY InvoiceNo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef2028",
   "metadata": {},
   "source": [
    "Window Functions\n",
    "\n",
    "A group-by takes data, and every row can go only into one grouping. A window function\n",
    "calculates a return value for every input row of a table based on a group of rows, called a frame.\n",
    "Each row can fall into one or more frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4dc5bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24165b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.select(\"*\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748148f0",
   "metadata": {},
   "source": [
    "first step to a window function is to create a window specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90ee635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "windowSpec = Window\\\n",
    ".partitionBy(\"CustomerId\", \"date\")\\\n",
    ".orderBy(desc(\"Quantity\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d111c851",
   "metadata": {},
   "source": [
    "Now we want to use an aggregation function to learn more about each specific customer.\n",
    "An example might be establishing the maximum purchase quantity over all time. To answer this, we\n",
    "use the same aggregation functions that we saw earlier by passing a column name or expression.\n",
    "In addition, we indicate the window specification that defines to which frames of data this\n",
    "function will apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7fb309cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76befea3",
   "metadata": {},
   "source": [
    "You will notice that this returns a column (or expressions). We can now use this in a DataFrame\n",
    "select statement. Before doing so, though, we will create the purchase quantity rank. To do that\n",
    "we use the dense_rank function to determine which date had the maximum purchase quantity\n",
    "for every customer. We use dense_rank as opposed to rank to avoid gaps in the ranking\n",
    "sequence when there are tied values (or in our case, duplicate rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b355c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank\n",
    "\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b5729",
   "metadata": {},
   "source": [
    "This also returns a column that we can use in select statements. Now we can perform a select to\n",
    "view the calculated window values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e98756ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 107:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|              74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|              74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|                 36|\n",
      "|     12347|2010-12-07|      30|           2|                2|                 36|\n",
      "|     12347|2010-12-07|      24|           3|                3|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    ".select(\n",
    "col(\"CustomerId\"),\n",
    "col(\"date\"),\n",
    "col(\"Quantity\"),\n",
    "purchaseRank.alias(\"quantityRank\"),\n",
    "purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28eef604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+----+-----+-----------+\n",
      "|CustomerId|      date|Quantity|rank|dRank|maxPurchase|\n",
      "+----------+----------+--------+----+-----+-----------+\n",
      "|     12346|2011-01-18|   74215|   1|    1|      74215|\n",
      "|     12346|2011-01-18|  -74215|   2|    2|      74215|\n",
      "|     12347|2010-12-07|      36|   1|    1|         36|\n",
      "|     12347|2010-12-07|      30|   2|    2|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      24|   3|    3|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|       6|  17|    5|         36|\n",
      "|     12347|2010-12-07|       6|  17|    5|         36|\n",
      "+----------+----------+--------+----+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\" SELECT CustomerId, date, Quantity,\\\n",
    "                    rank(Quantity) OVER (PARTITION BY CustomerId, date\\\n",
    "                                            ORDER BY Quantity DESC NULLS LAST\\\n",
    "                                            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as rank,\\\n",
    "                    dense_rank(Quantity) OVER (PARTITION BY CustomerId, date\\\n",
    "                                            ORDER BY Quantity DESC NULLS LAST\\\n",
    "                                            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as dRank,\\\n",
    "                    max(Quantity) OVER (PARTITION BY CustomerId, date\\\n",
    "                                            ORDER BY Quantity DESC NULLS LAST\\\n",
    "                                            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as maxPurchase\\\n",
    "            FROM dfWithDate WHERE CustomerId IS NOT NULL ORDER BY CustomerId\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28abe2e5",
   "metadata": {},
   "source": [
    "Grouping Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89620028",
   "metadata": {},
   "source": [
    "Sometimes we want something a bit more complete—an aggregation across multiple groups. \n",
    "We achieve this by using grouping sets.\n",
    "Grouping sets are a low-level tool for combining sets o0f aggregations together. \n",
    "They give you the ability to create arbitrary aggregation in their group-by statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f0e7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d482c541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "|     18287|   85039A|           96|\n",
      "|     18287|    84920|            4|\n",
      "+----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT CustomerId, stockCode, sum(Quantity) \\\n",
    "            FROM dfNoNull\\\n",
    "            GROUP BY customerId, stockCode\\\n",
    "            ORDER BY CustomerId DESC, stockCode DESC\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0401a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|customerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "|     18287|   85039A|           96|\n",
      "|     18287|    84920|            4|\n",
      "+----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\\\n",
    "  SELECT CustomerId, stockCode, sum(Quantity) \\\n",
    "    FROM dfNoNull \\\n",
    "GROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode)) \\\n",
    "ORDER BY CustomerId DESC, stockCode DESC\\\n",
    "\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65156f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b5e2381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 163:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|customerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "|     18287|   85039A|           96|\n",
      "|     18287|    84920|            4|\n",
      "+----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\\\n",
    "  SELECT CustomerId, stockCode, sum(Quantity) \\\n",
    "    FROM dfNoNull \\\n",
    "GROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode),()) \\\n",
    "ORDER BY CustomerId DESC, stockCode DESC\\\n",
    "\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d3687",
   "metadata": {},
   "source": [
    "Rollups\n",
    "\n",
    "Multidimensional aggregation that performs a variety of group-by style calculations for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3b53ae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 169:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      null|          null|       5176450|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|United Kingdom|         23949|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    ".selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\\\n",
    ".orderBy(\"Date\")\n",
    "\n",
    "rolledUpDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ddb4b3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 175:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+\n",
      "|      Date|Country|total_quantity|\n",
      "+----------+-------+--------------+\n",
      "|      null|   null|       5176450|\n",
      "|2010-12-01|   null|         26814|\n",
      "|2010-12-02|   null|         21023|\n",
      "|2010-12-03|   null|         14830|\n",
      "|2010-12-05|   null|         16395|\n",
      "+----------+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Country IS NULL\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "51430ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 178:>                                                        (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------+\n",
      "|Date|Country|total_quantity|\n",
      "+----+-------+--------------+\n",
      "|null|   null|       5176450|\n",
      "+----+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Date IS NULL\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b0507f",
   "metadata": {},
   "source": [
    "Cube\n",
    "\n",
    "A cube takes the rollup to a level deeper. Rather than treating elements hierarchically, a cube\n",
    "does the same thing across all dimensions. This means that it won’t just go by date over the\n",
    "entire time period, but also the country. To pose this as a question again, can you make a table\n",
    "that includes the following?\n",
    "\n",
    "    The total across all dates and countries\n",
    "    The total for each date across all countries\n",
    "    The total for each country on each date\n",
    "    The total for each country across all dates\n",
    "\n",
    "The method call is quite similar, but instead of calling rollup, we call cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a4bde788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 181:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-------------+\n",
      "|Date|  Country|sum(Quantity)|\n",
      "+----+---------+-------------+\n",
      "|null|  Finland|        10666|\n",
      "|null|   Greece|         1556|\n",
      "|null|Lithuania|          652|\n",
      "|null|   Poland|         3653|\n",
      "|null|  Iceland|         2458|\n",
      "+----+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\")))\\\n",
    ".select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c09d4",
   "metadata": {},
   "source": [
    "Pivot\n",
    "\n",
    "Pivots make it possible for you to convert a row into a column. \n",
    "For example, in our current data we have a Country column. With a pivot, \n",
    "we can aggregate according to some function for each of those given countries \n",
    "and display them in an easy-to-query way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6cf173d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "55288d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 208:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|      date|USA_sum(Quantity)|\n",
      "+----------+-----------------+\n",
      "|2011-12-06|             null|\n",
      "|2011-12-09|             null|\n",
      "|2011-12-08|             -196|\n",
      "|2011-12-07|             null|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"date > '2011-12-05'\").select(\"date\" ,\"`USA_sum(Quantity)`\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678208e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
