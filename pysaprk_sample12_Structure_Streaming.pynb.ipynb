{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e585834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/23 02:56:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").\\\n",
    "                                     appName(\"spark_on_docker\").\\\n",
    "                                     getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b7d504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "static = spark.read.json(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/activity-data/\")\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819f415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|           y|           z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "|1424686735090|1424686733090638193|nexus4_1|   18|nexus4|   g|stand| 3.356934E-4|-5.645752E-4|-0.018814087|\n",
      "|1424686735292|1424688581345918092|nexus4_2|   66|nexus4|   g|stand|-0.005722046| 0.029083252| 0.005569458|\n",
      "|1424686735500|1424686733498505625|nexus4_1|   99|nexus4|   g|stand|   0.0078125|-0.017654419| 0.010025024|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e647b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c78b6",
   "metadata": {},
   "source": [
    "Create Streaming DataFrames \n",
    "\n",
    "Streaming DataFrames are largely the same as static DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae74fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
    ".json(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/activity-data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a256c",
   "metadata": {},
   "source": [
    "Transformations on them to get our data into the correct format. \n",
    "\n",
    "Specify transformations on our streaming DataFrame before finally calling an action to start the stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aed6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts = streaming.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027b23e",
   "metadata": {},
   "source": [
    "Because this code is being written in local mode on a small machine, we are going to set the\n",
    "shuffle partitions to a small value to avoid creating too many shuffle partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e19568cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09865802",
   "metadata": {},
   "source": [
    "Need only to specify our action to start the query. \n",
    "\n",
    "Specify an output destination, or output sink for ourresult of this query. For this basic example, we are going to write to a memory sink which keeps an in-memory table of the results.\n",
    "\n",
    "In the process of specifying this sink, we’re going to need to define how Spark will output that data. In this example, we use the complete output mode. This mode rewrites all of the keys along with their counts after every trigger:\n",
    "\n",
    "We are now writing out our stream! You’ll notice that we set a unique query name to represent\n",
    "this stream, in this case activity_counts. We specified our format as an in-memory table and\n",
    "we set the output mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc12ed64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/23 04:33:36 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d3a26e7d-ec45-453a-80a3-2d4099e23807. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/12/23 04:33:36 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "21/12/23 04:33:42 WARN FileStreamSource: Listed 80 file(s) in 5674 ms\n",
      "[Stage 2:==============>                                            (1 + 3) / 4]\r"
     ]
    }
   ],
   "source": [
    "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n",
    ".format(\"memory\").outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea4531",
   "metadata": {},
   "source": [
    "After this code is executed, the streaming computation will have started in the background. \n",
    "\n",
    "The query object is a handle to that active streaming query, and we must specify that we would like\n",
    "to wait for the termination of the query using activityQuery.awaitTermination() to prevent\n",
    "the driver process from exiting while the query is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3095c3",
   "metadata": {},
   "source": [
    "Spark lists this stream, and other active ones, under the active streams in our SparkSession. We\n",
    "can see a list of those streams by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5242ab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7f30bc0b4730>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c827f00",
   "metadata": {},
   "source": [
    "Spark also assigns each stream a UUID, so if need be you could iterate through the list of\n",
    "running streams and select the above one. In this case, we assigned it to a variable, so that’s not\n",
    "necessary.\n",
    "\n",
    "Now that this stream is running, we can experiment with the results by querying the in-memory\n",
    "table it is maintaining of the current output of our streaming aggregation. This table will be\n",
    "called activity_counts, the same as the stream. To see the current data in this output table, we\n",
    "simply need to query it! We’ll do this in a simple loop that will print the results of the streaming\n",
    "query every second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be6d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "for x in range(5):\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    sleep(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
