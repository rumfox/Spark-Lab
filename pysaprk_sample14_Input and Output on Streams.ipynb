{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e585834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/03 05:33:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/01/03 05:33:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/01/03 05:33:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "#create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").\\\n",
    "                                     appName(\"spark_on_docker\").\\\n",
    "                                     getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b7d504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "static = spark.read.json(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/activity-data/\")\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819f415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|           y|           z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "|1424686735090|1424686733090638193|nexus4_1|   18|nexus4|   g|stand| 3.356934E-4|-5.645752E-4|-0.018814087|\n",
      "|1424686735292|1424688581345918092|nexus4_2|   66|nexus4|   g|stand|-0.005722046| 0.029083252| 0.005569458|\n",
      "|1424686735500|1424686733498505625|nexus4_1|   99|nexus4|   g|stand|   0.0078125|-0.017654419| 0.010025024|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e647b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c78b6",
   "metadata": {},
   "source": [
    "Create Streaming DataFrames \n",
    "\n",
    "Streaming DataFrames are largely the same as static DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae74fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
    ".json(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/activity-data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a256c",
   "metadata": {},
   "source": [
    "Transformations on them to get our data into the correct format. \n",
    "\n",
    "Specify transformations on our streaming DataFrame before finally calling an action to start the stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aed6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts = streaming.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027b23e",
   "metadata": {},
   "source": [
    "Because this code is being written in local mode on a small machine, we are going to set the\n",
    "shuffle partitions to a small value to avoid creating too many shuffle partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19568cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09865802",
   "metadata": {},
   "source": [
    "Need only to specify our action to start the query. \n",
    "\n",
    "Specify an output destination, or output sink for ourresult of this query. For this basic example, we are going to write to a memory sink which keeps an in-memory table of the results.\n",
    "\n",
    "In the process of specifying this sink, we’re going to need to define how Spark will output that data. In this example, we use the complete output mode. This mode rewrites all of the keys along with their counts after every trigger:\n",
    "\n",
    "We are now writing out our stream! You’ll notice that we set a unique query name to represent\n",
    "this stream, in this case activity_counts. We specified our format as an in-memory table and\n",
    "we set the output mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n",
    ".format(\"memory\").outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea4531",
   "metadata": {},
   "source": [
    "After this code is executed, the streaming computation will have started in the background. \n",
    "\n",
    "The query object is a handle to that active streaming query, and we must specify that we would like\n",
    "to wait for the termination of the query using activityQuery.awaitTermination() to prevent\n",
    "the driver process from exiting while the query is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37e6daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3095c3",
   "metadata": {},
   "source": [
    "Spark lists this stream, and other active ones, under the active streams in our SparkSession. We\n",
    "can see a list of those streams by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5242ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c827f00",
   "metadata": {},
   "source": [
    "Spark also assigns each stream a UUID, so if need be you could iterate through the list of\n",
    "running streams and select the above one. In this case, we assigned it to a variable, so that’s not\n",
    "necessary.\n",
    "\n",
    "Now that this stream is running, we can experiment with the results by querying the in-memory\n",
    "table it is maintaining of the current output of our streaming aggregation. This table will be\n",
    "called activity_counts, the same as the stream. To see the current data in this output table, we\n",
    "simply need to query it! We’ll do this in a simple loop that will print the results of the streaming\n",
    "query every second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be6d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "for x in range(1):\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show(3)\n",
    "    sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6349f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/03 05:23:33 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b03a4426-b2a4-4d8c-8a3d-e9bf73b5b6ab. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/01/03 05:23:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "simpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n",
    "    .where(\"stairs\")\\\n",
    "    .where(\"gt is not null\")\\\n",
    "    .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"simple_transform\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97342e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/03 05:24:08 WARN TaskSetManager: Stage 20 contains a task of very large size (2419 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------------+-------------------+\n",
      "|      gt| model| arrival_time|      creation_time|\n",
      "+--------+------+-------------+-------------------+\n",
      "|stairsup|nexus4|1424687983719|1424687981726802718|\n",
      "|stairsup|nexus4|1424687984000|1424687982009853255|\n",
      "|stairsup|nexus4|1424687984404|1424687982411977009|\n",
      "+--------+------+-------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM simple_transform\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c6b9e",
   "metadata": {},
   "source": [
    "Aggregations\n",
    "\n",
    "Structured Streaming has excellent support for aggregations. You can specify arbitrary\n",
    "aggregations, as you saw in the Structured APIs. For example, you can use a more exotic\n",
    "aggregation, like a cube, on the phone model and activity and the average x, y, z accelerations of\n",
    "our sensor (jump back to Chapter 7 in order to see potential aggregations that you can run on\n",
    "your stream):\n",
    "\n",
    "py4j error : at the container terminal  \n",
    "    \n",
    "▶ Solution 1 : \n",
    "    SOLVED: py4j.protocol.Py4JError: org.apache.spark.api.python.PythonUtils.getEncryptionEnabled does not exist in the JVM\n",
    "    출처: <https://sparkbyexamples.com/pyspark/pyspark-py4j-protocol-py4jerror-org-apache-spark-api-python-pythonutils-jvm/> \n",
    "\n",
    "    $pip install findspark\n",
    "\n",
    "    import findspark\n",
    "    findspark.init() \n",
    "\n",
    "▶ Solution 2 : \n",
    "    https://dhkdn9192.github.io/apache-spark/pyspark-py4j-error/\n",
    "\n",
    "    $ pip install py4j==0.10.9.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55776222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "import findspark\n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1015950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/03 05:35:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-739b9aa6-6bb4-42ed-a51e-c84d758e6985. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/01/03 05:35:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "deviceModelStats = streaming.cube('gt', 'Model', 'Device').avg()\\\n",
    "    .drop(\"avg(Arrival_Time)\")\\\n",
    "    .drop(\"avg(Creation_Time)\")\\\n",
    "    .drop(\"avg(Index)\")\\\n",
    "    .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1375a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM device_counts\").show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc034236",
   "metadata": {},
   "source": [
    "Joins\n",
    "\n",
    "As of Apache Spark 2.2, Structured Streaming supports joining streaming DataFrames to static\n",
    "DataFrames. Spark 2.3 will add the ability to join multiple streams together. You can do multiple\n",
    "column joins and supplement streaming data with that from static data sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38bafa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/03 05:44:46 WARN FileStreamSource: Listed 80 file(s) in 2685 ms\n",
      "22/01/03 05:44:48 WARN FileStreamSource: Listed 80 file(s) in 2014 ms\n"
     ]
    }
   ],
   "source": [
    "historicalAgg = static.groupBy(\"gt\", \"model\").avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c679c523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/03 05:47:21 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-eb03e93b-45d8-4e35-8bb0-2f1b75d993c7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/01/03 05:47:21 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/01/03 05:47:22 WARN FileStreamSource: Listed 80 file(s) in 2351 ms\n",
      "22/01/03 05:47:23 WARN FileStreamSource: Listed 80 file(s) in 2059 ms\n"
     ]
    }
   ],
   "source": [
    "deviceModelStats = streaming.drop(\"Arrival_Time\", \"Creation_Time\", \"Index\")\\\n",
    ".cube(\"gt\", \"model\").avg()\\\n",
    ".join(historicalAgg, [\"gt\", \"model\"])\\\n",
    ".writeStream.queryName(\"device_counts2\").format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
