{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e585834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").\\\n",
    "                                     appName(\"spark_on_docker\").\\\n",
    "                                     getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "static = spark.read.json(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/activity-data\")\n",
    "\n",
    "streaming = spark\\\n",
    "    .readStream\\\n",
    "    .schema(static.schema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 10)\\\n",
    "    .json(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/activity-data\")\n",
    "\n",
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38118a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "withEventTime = streaming.selectExpr(\"*\",\"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb523294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\")).count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevents_per_window\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05edd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69503ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    ", this does apply to any window-style aggregation (or stateful\n",
    "computation) we would like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\"), \"User\").count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"pyevents_per_window2\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window2\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0fa0f3",
   "metadata": {},
   "source": [
    "Of importance is the fact that we can also perform an aggregation on multiple columns, including the event time column. Just like we saw in the previous chapter, we can even perform these aggregations using methods like cube. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2516e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\\\n",
    ".count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevents_per_window\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96665186",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window2\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e84f78c",
   "metadata": {},
   "source": [
    "+--------------------+----+------+\n",
    "|              window|User| count|\n",
    "+--------------------+----+------+\n",
    "|{2015-02-24 12:20...|   f|133623|\n",
    "|{2015-02-24 13:00...|   f| 33366|\n",
    "|{2015-02-24 14:50...|   e|126282|\n",
    "|{2015-02-23 14:30...|   h| 94669|\n",
    "|{2015-02-24 14:10...|   e| 67577|\n",
    "+--------------------+----+------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe29645b",
   "metadata": {},
   "source": [
    "In this example, we have 10-minute windows, starting every five minutes. \n",
    "\n",
    "Therefore each event will fall into two different windows. You can tweak this further according to your needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\\\n",
    ".count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevents_per_window3\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf059ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window3\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da934e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window3\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd94a8aa",
   "metadata": {},
   "source": [
    "+--------------------+------+\n",
    "|              window| count|\n",
    "+--------------------+------+\n",
    "|{2015-02-23 14:15...|107668|\n",
    "|{2015-02-24 11:50...|150773|\n",
    "|{2015-02-24 13:00...|133323|\n",
    "|{2015-02-22 00:35...|    35|\n",
    "|{2015-02-23 12:30...|100853|\n",
    "+--------------------+------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f396d",
   "metadata": {},
   "source": [
    "Handling Late Data with Watermarks\n",
    "\n",
    "Concretely, a watermark is an amount of time following a given event or set of events after which we do not expect to see any more data from that time. We know this can happen due to delays on the network, devices that lose a connection, or any number of other issues.\n",
    "\n",
    " If we specify a watermark of 10 minutes. When doing this, we instruct Spark that any event that occurs more than 10 “event-time” minutes past a previous event should be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "withEventTime\\\n",
    "    .withWatermark(\"event_time\", \"30 minutes\")\\\n",
    "    .groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\\\n",
    "    .count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"pyevents_per_window4\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe4216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window4\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c49a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window4\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93029eab",
   "metadata": {},
   "source": [
    "+--------------------+-----+\n",
    "|              window|count|\n",
    "+--------------------+-----+\n",
    "|{2015-02-23 14:15...|26936|\n",
    "|{2015-02-24 11:50...|37714|\n",
    "|{2015-02-24 13:00...|33324|\n",
    "|{2015-02-22 00:35...|    6|\n",
    "|{2015-02-23 12:30...|25218|\n",
    "+--------------------+-----+\n",
    "only showing top 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67c97b",
   "metadata": {},
   "source": [
    "Dropping Duplicates in a Stream\n",
    "\n",
    "One of the more difficult operations in record-at-a-time systems is removing duplicates from the stream.  ...  A perfect example of this are Internet of Things (IoT) applications that have upstream producers generating messages in nonstable network environments, and the same message might end up being sent multiple times. Your downstream applications and aggregations should be able to assume that there is only one of each message.\n",
    "\n",
    "\n",
    "Essentially, Structured Streaming makes it easy to take message systems that provide at-least-once semantics, and convert them into exactly-once by dropping duplicate messages as they come in, based on arbitrary keys. To de-duplicate data, Spark will maintain a number of user specified keys and ensure that duplicates are ignored.\n",
    "\n",
    "The core assumption is that duplicate events will have the same timestamp as well as identifier. In this model, rows with two different timestamps are two different records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "withEventTime\\\n",
    "    .withWatermark(\"event_time\", \"5 seconds\")\\\n",
    "    .dropDuplicates([\"User\", \"event_time\"])\\\n",
    "    .groupBy(\"User\")\\\n",
    "    .count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"pydeduplicated\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from pydeduplicated\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fdb70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from pydeduplicated\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359bcf9b",
   "metadata": {},
   "source": [
    "+----+-----+\n",
    "|User|count|\n",
    "+----+-----+\n",
    "|   a|80850|\n",
    "|   b|91230|\n",
    "|   c|77150|\n",
    "|   g|91679|\n",
    "|   h|77330|\n",
    "|   e|96897|\n",
    "|   f|92060|\n",
    "|   d|81240|\n",
    "|   i|92550|\n",
    "+----+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b125ff6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
