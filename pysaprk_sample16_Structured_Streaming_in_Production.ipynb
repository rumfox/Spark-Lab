{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e585834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").\\\n",
    "                                     appName(\"spark_on_docker\").\\\n",
    "                                     getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9f1b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "static = spark.read.json(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/activity-data\")\n",
    "\n",
    "streaming = spark\\\n",
    "    .readStream\\\n",
    "    .schema(static.schema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 10)\\\n",
    "    .json(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/activity-data\")\n",
    "\n",
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f38118a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "withEventTime = streaming.selectExpr(\"*\",\"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb523294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\")).count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevents_per_window\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05edd4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 02:04:03 WARN FileStreamSource: Listed 80 file(s) in 5822 ms\n",
      "[Stage 14:=============================================>           (8 + 2) / 10]\r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69503ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    ", this does apply to any window-style aggregation (or stateful\n",
    "computation) we would like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e85c73f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 05:28:00 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9d52c2a9-edf6-4ca6-a46b-bd6a1bffb091. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/01/18 05:28:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f713022a400>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 05:28:01 WARN FileStreamSource: Listed 80 file(s) in 5565 ms\n",
      "22/01/18 05:28:07 WARN FileStreamSource: Listed 80 file(s) in 4882 ms + 2) / 10]\n",
      "22/01/18 05:28:13 WARN FileStreamSource: Listed 80 file(s) in 5512 ms + 8) / 10]\n",
      "22/01/18 05:28:17 WARN FileStreamSource: Listed 80 file(s) in 4111 ms + 8) / 10]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\"), \"User\").count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"pyevents_per_window2\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7598de21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 05:28:25 WARN FileStreamSource: Listed 80 file(s) in 3931 ms + 2) / 10]\n",
      "22/01/18 05:28:30 WARN FileStreamSource: Listed 80 file(s) in 4488 ms + 2) / 10]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window2\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0fa0f3",
   "metadata": {},
   "source": [
    "Of importance is the fact that we can also perform an aggregation on multiple columns, including the event time column. Just like we saw in the previous chapter, we can even perform these aggregations using methods like cube. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2516e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\\\n",
    ".count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevents_per_window\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96665186",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window2\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e84f78c",
   "metadata": {},
   "source": [
    "+--------------------+----+------+\n",
    "|              window|User| count|\n",
    "+--------------------+----+------+\n",
    "|{2015-02-24 12:20...|   f|133623|\n",
    "|{2015-02-24 13:00...|   f| 33366|\n",
    "|{2015-02-24 14:50...|   e|126282|\n",
    "|{2015-02-23 14:30...|   h| 94669|\n",
    "|{2015-02-24 14:10...|   e| 67577|\n",
    "+--------------------+----+------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe29645b",
   "metadata": {},
   "source": [
    "In this example, we have 10-minute windows, starting every five minutes. \n",
    "\n",
    "Therefore each event will fall into two different windows. You can tweak this further according to your needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fbe235a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 05:50:35 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f9cfc10a-9af1-4981-9544-356cfb9f4588. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/01/18 05:50:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f71301d9940>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 05:50:36 WARN FileStreamSource: Listed 80 file(s) in 2536 ms\n",
      "22/01/18 05:50:37 WARN FileStreamSource: Listed 80 file(s) in 2532 ms\n",
      "22/01/18 05:50:38 WARN FileStreamSource: Listed 80 file(s) in 2552 ms\n",
      "22/01/18 05:50:39 WARN FileStreamSource: Listed 80 file(s) in 2961 ms\n",
      "22/01/18 05:50:42 WARN FileStreamSource: Listed 80 file(s) in 4724 ms + 8) / 10]\n",
      "22/01/18 05:50:45 WARN FileStreamSource: Listed 80 file(s) in 5587 ms + 2) / 10]\n",
      "22/01/18 05:50:46 WARN FileStreamSource: Listed 80 file(s) in 4372 ms           \n",
      "22/01/18 05:50:50 WARN FileStreamSource: Listed 80 file(s) in 5015 ms + 8) / 10]\n",
      "22/01/18 05:50:51 WARN FileStreamSource: Listed 80 file(s) in 4454 ms + 2) / 10]\n",
      "22/01/18 05:50:53 WARN FileStreamSource: Listed 80 file(s) in 2497 ms           \n",
      "22/01/18 05:50:54 WARN FileStreamSource: Listed 80 file(s) in 3222 ms + 8) / 10]\n",
      "22/01/18 05:50:56 WARN FileStreamSource: Listed 80 file(s) in 3697 ms           \n",
      "22/01/18 05:50:58 WARN FileStreamSource: Listed 80 file(s) in 3902 ms\n",
      "22/01/18 05:51:01 WARN FileStreamSource: Listed 80 file(s) in 4534 ms + 2) / 10]\n",
      "22/01/18 05:51:02 WARN FileStreamSource: Listed 80 file(s) in 4140 ms           \n",
      "22/01/18 05:51:06 WARN FileStreamSource: Listed 80 file(s) in 4715 ms + 8) / 10]\n",
      "22/01/18 05:51:06 WARN FileStreamSource: Listed 80 file(s) in 4297 ms + 2) / 10]\n",
      "22/01/18 05:51:09 WARN FileStreamSource: Listed 80 file(s) in 3701 ms           \n",
      "22/01/18 05:51:11 WARN FileStreamSource: Listed 80 file(s) in 4419 ms + 8) / 10]\n",
      "22/01/18 05:51:14 WARN FileStreamSource: Listed 80 file(s) in 4358 ms           \n",
      "22/01/18 05:51:14 WARN FileStreamSource: Listed 80 file(s) in 3623 ms\n",
      "22/01/18 05:51:19 WARN FileStreamSource: Listed 80 file(s) in 5212 ms + 2) / 10]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\\\n",
    ".count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pyevents_per_window3\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdf059ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 05:52:13 WARN FileStreamSource: Listed 80 file(s) in 2041 ms\n",
      "22/01/18 05:52:14 WARN FileStreamSource: Listed 80 file(s) in 2029 ms\n",
      "22/01/18 05:52:15 WARN FileStreamSource: Listed 80 file(s) in 2344 ms\n",
      "22/01/18 05:52:16 WARN FileStreamSource: Listed 80 file(s) in 2410 ms\n",
      "22/01/18 05:52:17 WARN FileStreamSource: Listed 80 file(s) in 2617 ms\n",
      "22/01/18 05:52:18 WARN FileStreamSource: Listed 80 file(s) in 2911 ms\n",
      "22/01/18 05:52:19 WARN FileStreamSource: Listed 80 file(s) in 3155 ms\n",
      "22/01/18 05:52:20 WARN FileStreamSource: Listed 80 file(s) in 3422 ms\n",
      "22/01/18 05:52:22 WARN FileStreamSource: Listed 80 file(s) in 3563 ms\n",
      "22/01/18 05:52:23 WARN FileStreamSource: Listed 80 file(s) in 3743 ms\n",
      "22/01/18 05:52:24 WARN FileStreamSource: Listed 80 file(s) in 3169 ms\n",
      "22/01/18 05:52:25 WARN FileStreamSource: Listed 80 file(s) in 2843 ms\n",
      "22/01/18 05:52:26 WARN FileStreamSource: Listed 80 file(s) in 2478 ms\n",
      "22/01/18 05:52:26 WARN FileStreamSource: Listed 80 file(s) in 2513 ms\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window3\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da934e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window3\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd94a8aa",
   "metadata": {},
   "source": [
    "+--------------------+------+\n",
    "|              window| count|\n",
    "+--------------------+------+\n",
    "|{2015-02-23 14:15...|107668|\n",
    "|{2015-02-24 11:50...|150773|\n",
    "|{2015-02-24 13:00...|133323|\n",
    "|{2015-02-22 00:35...|    35|\n",
    "|{2015-02-23 12:30...|100853|\n",
    "+--------------------+------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f396d",
   "metadata": {},
   "source": [
    "Handling Late Data with Watermarks\n",
    "\n",
    "Concretely, a watermark is an amount of time following a given event or set of events after which we do not expect to see any more data from that time. We know this can happen due to delays on the network, devices that lose a connection, or any number of other issues.\n",
    "\n",
    " If we specify a watermark of 10 minutes. When doing this, we instruct Spark that any event that occurs more than 10 “event-time” minutes past a previous event should be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70c5571f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 06:28:54 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-18e732e5-b69a-49ec-9292-3901b02f7d0a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/01/18 06:28:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f71301d9cd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 06:28:57 WARN FileStreamSource: Listed 80 file(s) in 2089 ms + 8) / 10]\n",
      "22/01/18 06:29:01 WARN FileStreamSource: Listed 80 file(s) in 4080 ms + 2) / 10]\n",
      "22/01/18 06:29:01 WARN FileStreamSource: Listed 80 file(s) in 4102 ms\n",
      "22/01/18 06:29:01 WARN FileStreamSource: Listed 80 file(s) in 3710 ms + 1) / 10]\n",
      "22/01/18 06:29:04 WARN FileStreamSource: Listed 80 file(s) in 3193 ms + 8) / 10]\n",
      "22/01/18 06:29:04 WARN FileStreamSource: Listed 80 file(s) in 3070 ms\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "withEventTime\\\n",
    "    .withWatermark(\"event_time\", \"30 minutes\")\\\n",
    "    .groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\\\n",
    "    .count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"pyevents_per_window4\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fe4216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 06:29:05 WARN FileStreamSource: Listed 80 file(s) in 4059 ms + 2) / 10]\n",
      "22/01/18 06:29:07 WARN FileStreamSource: Listed 80 file(s) in 2899 ms           \n",
      "22/01/18 06:29:07 WARN FileStreamSource: Listed 80 file(s) in 2963 ms\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window4\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24c49a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pyevents_per_window4\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93029eab",
   "metadata": {},
   "source": [
    "+--------------------+-----+\n",
    "|              window|count|\n",
    "+--------------------+-----+\n",
    "|{2015-02-23 14:15...|26936|\n",
    "|{2015-02-24 11:50...|37714|\n",
    "|{2015-02-24 13:00...|33324|\n",
    "|{2015-02-22 00:35...|    6|\n",
    "|{2015-02-23 12:30...|25218|\n",
    "+--------------------+-----+\n",
    "only showing top 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67c97b",
   "metadata": {},
   "source": [
    "Dropping Duplicates in a Stream\n",
    "\n",
    "One of the more difficult operations in record-at-a-time systems is removing duplicates from the stream.  ...  A perfect example of this are Internet of Things (IoT) applications that have upstream producers generating messages in nonstable network environments, and the same message might end up being sent multiple times. Your downstream applications and aggregations should be able to assume that there is only one of each message.\n",
    "\n",
    "\n",
    "Essentially, Structured Streaming makes it easy to take message systems that provide at-least-once semantics, and convert them into exactly-once by dropping duplicate messages as they come in, based on arbitrary keys. To de-duplicate data, Spark will maintain a number of user specified keys and ensure that duplicates are ignored.\n",
    "\n",
    "The core assumption is that duplicate events will have the same timestamp as well as identifier. In this model, rows with two different timestamps are two different records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "304a2fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 07:04:08 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7e0f9eca-e9c0-4244-89d7-41c3f765d11d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/01/18 07:04:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f713015ef70>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 07:04:08 WARN FileStreamSource: Listed 80 file(s) in 2032 ms\n",
      "22/01/18 07:04:09 WARN FileStreamSource: Listed 80 file(s) in 2030 ms\n",
      "22/01/18 07:04:09 WARN FileStreamSource: Listed 80 file(s) in 2137 ms\n",
      "22/01/18 07:04:10 WARN FileStreamSource: Listed 80 file(s) in 2370 ms\n",
      "22/01/18 07:04:13 WARN FileStreamSource: Listed 80 file(s) in 4055 ms + 8) / 10]\n",
      "22/01/18 07:04:14 WARN FileStreamSource: Listed 80 file(s) in 4019 ms + 2) / 10]\n",
      "22/01/18 07:04:14 WARN FileStreamSource: Listed 80 file(s) in 4024 ms\n",
      "22/01/18 07:04:15 WARN FileStreamSource: Listed 80 file(s) in 4607 ms\n",
      "22/01/18 07:04:16 WARN FileStreamSource: Listed 80 file(s) in 2535 ms\n",
      "22/01/18 07:04:17 WARN FileStreamSource: Listed 80 file(s) in 2824 ms0 + 5) / 5]\n",
      "22/01/18 07:04:20 WARN FileStreamSource: Listed 80 file(s) in 4676 ms + 8) / 10]\n",
      "22/01/18 07:04:21 WARN FileStreamSource: Listed 80 file(s) in 5568 ms\n",
      "22/01/18 07:04:22 WARN FileStreamSource: Listed 80 file(s) in 6042 ms\n",
      "22/01/18 07:04:22 WARN FileStreamSource: Listed 80 file(s) in 4724 ms\n",
      "22/01/18 07:04:24 WARN FileStreamSource: Listed 80 file(s) in 3943 ms + 2) / 10]\n",
      "22/01/18 07:04:25 WARN FileStreamSource: Listed 80 file(s) in 3975 ms           \n",
      "22/01/18 07:04:25 WARN FileStreamSource: Listed 80 file(s) in 3477 ms\n",
      "22/01/18 07:04:25 WARN FileStreamSource: Listed 80 file(s) in 3386 ms\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "withEventTime\\\n",
    "    .withWatermark(\"event_time\", \"5 seconds\")\\\n",
    "    .dropDuplicates([\"User\", \"event_time\"])\\\n",
    "    .groupBy(\"User\")\\\n",
    "    .count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"pydeduplicated\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5af0c1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/18 07:05:56 WARN FileStreamSource: Listed 80 file(s) in 3806 ms\n",
      "22/01/18 07:05:57 WARN FileStreamSource: Listed 80 file(s) in 3917 ms\n",
      "22/01/18 07:05:57 WARN FileStreamSource: Listed 80 file(s) in 3049 ms\n",
      "22/01/18 07:05:57 WARN FileStreamSource: Listed 80 file(s) in 3000 ms\n",
      "22/01/18 07:05:58 WARN FileStreamSource: Listed 80 file(s) in 2983 ms\n",
      "22/01/18 07:05:58 WARN FileStreamSource: Listed 80 file(s) in 2790 ms\n",
      "22/01/18 07:05:59 WARN FileStreamSource: Listed 80 file(s) in 2574 ms\n",
      "22/01/18 07:06:00 WARN FileStreamSource: Listed 80 file(s) in 2077 ms\n",
      "22/01/18 07:06:02 WARN FileStreamSource: Listed 80 file(s) in 2231 ms\n",
      "22/01/18 07:06:02 WARN FileStreamSource: Listed 80 file(s) in 2137 ms\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from pydeduplicated\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fdb70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from pydeduplicated\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359bcf9b",
   "metadata": {},
   "source": [
    "+----+-----+\n",
    "|User|count|\n",
    "+----+-----+\n",
    "|   a|80850|\n",
    "|   b|91230|\n",
    "|   c|77150|\n",
    "|   g|91679|\n",
    "|   h|77330|\n",
    "|   e|96897|\n",
    "|   f|92060|\n",
    "|   d|81240|\n",
    "|   i|92550|\n",
    "+----+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b125ff6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
