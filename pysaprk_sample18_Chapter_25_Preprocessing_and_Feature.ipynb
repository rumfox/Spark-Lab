{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e585834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/30 14:25:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").\\\n",
    "                                     appName(\"spark_on_docker\").\\\n",
    "                                     getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f8026a",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "    Formatting Models According to Your Use Case\n",
    "----------------------------------------------------\n",
    "\n",
    "To preprocess data for Spark’s different advanced analytics tools, you must consider your end objective. The following list walks through the requirements for input data structure for each advanced analytics task in MLlib:\n",
    "\n",
    "    - In the case of most classification and regression algorithms, you want to get your data into a column of type Double to represent the label and a column of type Vector (either dense or sparse) to represent the features.\n",
    "\n",
    "    - In the case of recommendation, you want to get your data into a column of users, a column of items (say movies or books), and a column of ratings.\n",
    "\n",
    "    - In the case of unsupervised learning, a column of type Vector (either dense or sparse) is needed to represent the features.\n",
    "\n",
    "    - In the case of graph analytics, you will want a DataFrame of vertices and a DataFrame of edges.\n",
    "\n",
    "\n",
    "The best way to get your data in these formats is through transformers. Transformers are functions that accept a DataFrame as an argument and return a new DataFrame as a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11bbaf",
   "metadata": {},
   "source": [
    "Before we proceed, we’re going to read in several different sample datasets, each of which has\n",
    "different properties we will manipulate in this chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc5a73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "sales = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\")\\\n",
    "    .coalesce(5)\\\n",
    "    .where(\"Description IS NOT NULL\")\n",
    "fakeIntDF = spark.read.parquet(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/simple-ml-integers\")\n",
    "simpleDF = spark.read.json(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/simple-ml\")\n",
    "scaleDF = spark.read.parquet(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/simple-ml-scaling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167446dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales.cache()\n",
    "sales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79e332",
   "metadata": {},
   "source": [
    "--------------------\n",
    "    Transformers\n",
    "--------------------\n",
    "\n",
    "Transformers are functions that convert raw data in some way. This might be to create a new interaction variable (from two other variables), to normalize a column, or to simply turn it into a Double to be input into a model. Transformers are primarily used in preprocessing or feature generation.\n",
    "\n",
    "Spark’s transformer only includes a transform method. This is because it will not change based on the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cae211",
   "metadata": {},
   "source": [
    "Estimators for Preprocessing\n",
    "\n",
    "An estimator is necessary when a transformation you would like to perform must be initialized with data or information about the input column (often derived by doing a pass over the input column itself). \n",
    "\n",
    "For example, if you wanted to scale the values in our column to have mean zero and unit variance, you would need to perform a pass over the entire data in order to calculate the values you would use to normalize the data to mean zero and unit variance. In effect, an estimator can be a transformer configured according to your particular input data. In simplest terms, you can either blindly apply a transformation (a “regular” transformer type) or perform a transformation based on your data (an estimator type).\n",
    "\n",
    "An example of this type of estimator is the StandardScaler, which scales your input column according to the range of values in that column to have a zero mean and a variance of 1 in each dimension. For that reason it must first perform a pass over the data to create the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02ab39",
   "metadata": {},
   "source": [
    "High-Level Transformers\n",
    "\n",
    "High-level transformers allow you to concisely specify a number of transformations in one. These operate at a “high level”, and allow you to avoid doing data manipulations or transformations one by one. \n",
    "\n",
    "In general, you should try to use the highest level transformers you can, in order to minimize the risk of error and help you focus on the business problem instead of the smaller details of implementation. While this is not always possible, it’s a good objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887a3d4",
   "metadata": {},
   "source": [
    "RFormula\n",
    "\n",
    "The RFormula is the easiest transfomer to use when you have “conventionally” formatted data. \n",
    "\n",
    "Spark borrows this transformer from the R language to make it simple to declaratively specify a set of transformations for your data.\n",
    "\n",
    "The RFormula will automatically handle categorical inputs (specified as strings) by performing something called one-hot encoding. \n",
    "\n",
    "With the RFormula, numeric columns will be cast to Double but will not be one-hot encoded. If the label column is of type String, it will be first transformed to Double with StringIndexer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea63d20",
   "metadata": {},
   "source": [
    "The RFormula allows you to specify your transformations in declarative syntax. It is simple to use once you understand the syntax. Currently, RFormula supports a limited subset of the R operators that in practice work quite well for simple transformations. The basic operators are:\n",
    "    \n",
    "    ~\n",
    "    Separate target and terms\n",
    "\n",
    "    +\n",
    "    Concatenate terms; “+ 0” means removing the intercept (this means the y-intercept of the line\n",
    "    that we will fit will be 0)\n",
    "    \n",
    "    -\n",
    "    Remove a term; “- 1” means removing intercept (this means the y-intercept of the line that\n",
    "    we will fit will be 0)\n",
    "    \n",
    "    :\n",
    "    Interaction (multiplication for numeric values, or binarized categorical values)\n",
    "    \n",
    "    .\n",
    "    All columns except the target/dependent variabl\n",
    "\n",
    "RFormula also uses default columns of label and features to label, you guessed it, the label and the set of features that it outputs (for supervised machine learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05844885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
      "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\n",
    "supervised.fit(simpleDF).transform(simpleDF).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf4829",
   "metadata": {},
   "source": [
    "SQL Transformers\n",
    "\n",
    "A SQLTransformer allows you to leverage Spark’s vast library of SQL-related manipulations just as you would a MLlib transformation. \n",
    "\n",
    "Any SELECT statement you can use in SQL is a valid transformation. The only thing you need to change is that instead of using the table name, you should just use the keyword THIS. \n",
    "\n",
    "You might want to use SQLTransformer if you want to formally codify some DataFrame manipulation as a preprocessing step, or try different SQL expressions for features during hyperparameter tuning. Also note that the output of this transformation will be appended as a column to the output DataFrame.\n",
    "\n",
    "You might want to use an SQLTransformer in order to represent all of your manipulations on the very rawest form of your data so you can version different variations of manipulations as transformers. This gives you the benefit of building and testing varying pipelines, all by simply swapping out transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a4f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|         1721|     119|   18180.0|\n",
      "|         1070|     107|   12782.0|\n",
      "|          701|      59|   17402.0|\n",
      "|          478|      35|   16642.0|\n",
      "|          477|      28|   16811.0|\n",
      "|          986|      71|   15053.0|\n",
      "|         1419|      50|   12913.0|\n",
      "|          445|      43|   12628.0|\n",
      "|         4505|     183|   14401.0|\n",
      "|          271|      20|   16851.0|\n",
      "+-------------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "basicTransformation = SQLTransformer()\\\n",
    "    .setStatement(\"\"\"\n",
    "                    SELECT sum(Quantity), count(*), CustomerID\n",
    "                    FROM __THIS__\n",
    "                    GROUP BY CustomerID\n",
    "    \"\"\")\n",
    "\n",
    "basicTransformation.transform(sales).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b708e",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "    VectorAssembler\n",
    "-----------------------\n",
    "\n",
    "The VectorAssembler is a tool you’ll use in nearly every single pipeline you generate. It helps concatenate all your features into one big vector you can then pass into an estimator. \n",
    "\n",
    "It’s used typically in the last step of a machine learning pipeline and takes as input a number of columns of Boolean, Double, or Vector. This is particularly helpful if you’re going to perform a number of manipulations using a variety of transformers and need to gather all of those results together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7510d2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_0ef02da753d5__output|\n",
      "+----+----+----+------------------------------------+\n",
      "|   1|   2|   3|                       [1.0,2.0,3.0]|\n",
      "|   4|   5|   6|                       [4.0,5.0,6.0]|\n",
      "|   7|   8|   9|                       [7.0,8.0,9.0]|\n",
      "+----+----+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d3141c",
   "metadata": {},
   "source": [
    "Working with Continuous Features\n",
    "\n",
    "Continuous features are just values on the number line, from positive infinity to negative infinity.\n",
    "\n",
    "There are two common transformers for continuous features. \n",
    "\n",
    "First, you can convert continuous features into categorical features via a process called bucketing, \n",
    "or you can scale and normalize your features according to several different requirements. \n",
    "\n",
    "These transformers will only work on Double types, so make sure you’ve turned any other numerical values to Double:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef5510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d8e3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|0.0|\n",
      "|1.0|\n",
      "|2.0|\n",
      "|3.0|\n",
      "|4.0|\n",
      "|5.0|\n",
      "|6.0|\n",
      "|7.0|\n",
      "|8.0|\n",
      "|9.0|\n",
      "+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a720d",
   "metadata": {},
   "source": [
    "-------------------\n",
    "    Bucketing\n",
    "-------------------\n",
    "\n",
    "The most straightforward approach to bucketing or binning is using the Bucketizer. This will split a given continuous feature into the buckets of your designation. You specify how buckets should be created via an array or list of Double values. This is useful because you may want to simplify the features in your dataset or simplify their representations for interpretation later on.\n",
    "\n",
    "For example, imagine you have a column that represents a person’s weight and you would like to predict some value based on this information. In some cases, it might be simpler to create three buckets of “overweight,” “average,” and “underweight.” \n",
    "\n",
    "To specify the bucket, set its borders. For example, setting splits to 5.0, 10.0, 250.0 on our contDF will actually fail because we don’t cover all possible input ranges. When specifying your bucket points, the values you pass into splits must satisfy three requirements:\n",
    "\n",
    "    - The minimum value in your splits array must be less than the minimum value in your DataFrame.\n",
    "    - The maximum value in your splits array must be greater than the maximum value in your DataFrame.\n",
    "    - You need to specify at a minimum three values in the splits array, which creates two buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d312ad",
   "metadata": {},
   "source": [
    "In addition to splitting based on hardcoded values, another option is to split based on percentiles in our data. This is done with QuantileDiscretizer, which will bucket the values into user\u0002specified buckets with the splits being determined by approximate quantiles values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26954dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+\n",
      "|  id|Bucketizer_9a66f2e0bad2__output|\n",
      "+----+-------------------------------+\n",
      "| 0.0|                            0.0|\n",
      "| 1.0|                            0.0|\n",
      "| 2.0|                            0.0|\n",
      "| 3.0|                            0.0|\n",
      "| 4.0|                            0.0|\n",
      "| 5.0|                            1.0|\n",
      "| 6.0|                            1.0|\n",
      "| 7.0|                            1.0|\n",
      "| 8.0|                            1.0|\n",
      "| 9.0|                            1.0|\n",
      "|10.0|                            2.0|\n",
      "|11.0|                            2.0|\n",
      "|12.0|                            2.0|\n",
      "|13.0|                            2.0|\n",
      "|14.0|                            2.0|\n",
      "|15.0|                            2.0|\n",
      "|16.0|                            2.0|\n",
      "|17.0|                            2.0|\n",
      "|18.0|                            2.0|\n",
      "|19.0|                            2.0|\n",
      "+----+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0af08bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------------+\n",
      "|  id|QuantileDiscretizer_996a5ef244b4__output|\n",
      "+----+----------------------------------------+\n",
      "| 0.0|                                     0.0|\n",
      "| 1.0|                                     0.0|\n",
      "| 2.0|                                     0.0|\n",
      "| 3.0|                                     1.0|\n",
      "| 4.0|                                     1.0|\n",
      "| 5.0|                                     1.0|\n",
      "| 6.0|                                     1.0|\n",
      "| 7.0|                                     2.0|\n",
      "| 8.0|                                     2.0|\n",
      "| 9.0|                                     2.0|\n",
      "|10.0|                                     2.0|\n",
      "|11.0|                                     3.0|\n",
      "|12.0|                                     3.0|\n",
      "|13.0|                                     3.0|\n",
      "|14.0|                                     3.0|\n",
      "|15.0|                                     4.0|\n",
      "|16.0|                                     4.0|\n",
      "|17.0|                                     4.0|\n",
      "|18.0|                                     4.0|\n",
      "|19.0|                                     4.0|\n",
      "+----+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "bucketer = QuantileDiscretizer().setNumBuckets(5).setInputCol(\"id\")\n",
    "fittedBucketer = bucketer.fit(contDF)\n",
    "fittedBucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6837f19",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "    Scaling and Normalization\n",
    "-----------------------------------\n",
    "\n",
    "You might want to do this when your data contains a number of columns based on different scales. \n",
    "\n",
    "For instance, say we have a DataFrame with two columns: weight (in ounces) and height (in feet). If you don’t scale or normalize, the algorithm will be less sensitive to variations in height because height values in feet are much lower than weight values in ounces. That’s an example where you should scale your data. \n",
    "\n",
    "An example of normalization might involve transforming the data so that each point’s value is a representation of its distance from the mean of that column.\n",
    "\n",
    "In MLlib, this is always done on columns of type Vector. MLlib will look across all the rows in a given column (of type Vector) and then treat every dimension in those vectors as its own particular column. It will then apply the scaling or normalization function on each dimension separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b30c53",
   "metadata": {},
   "source": [
    "----------------------\n",
    "    StandardScaler\n",
    "----------------------\n",
    "\n",
    "The StandardScaler standardizes a set of features to have zero mean and a standard deviation of 1. The flag withStd will scale the data to unit standard deviation while the flag withMean (false by default) will center the data prior to scaling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51b2a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------+\n",
      "| id|      features|StandardScaler_8084f6515a06__output|\n",
      "+---+--------------+-----------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
      "+---+--------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c912a11",
   "metadata": {},
   "source": [
    "MinMaxScaler\n",
    "\n",
    "The MinMaxScaler will scale the values in a vector (component wise) to the proportional values on a scale from a given min value to a max value. If you specify the minimum value to be 0 and the maximum value to be 1, then all the values will fall in between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "168b4f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MinMaxScaler_738fe0e294de__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  1|[3.0,10.1,3.0]|                 [10.0,10.0,10.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
    "fittedminMax = minMax.fit(scaleDF)\n",
    "fittedminMax.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba4fd0",
   "metadata": {},
   "source": [
    "MaxAbsScaler\n",
    "\n",
    "The max absolute scaler (MaxAbsScaler) scales the data by dividing each value by the maximum absolute value in this feature. All values therefore end up between −1 and 1. This transformer does not shift or center the data at all in the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27398a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MaxAbsScaler_e022b84bd9a3__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  1|[3.0,10.1,3.0]|                    [1.0,1.0,1.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "maScaler = MaxAbsScaler().setInputCol(\"features\")\n",
    "fittedmaScaler = maScaler.fit(scaleDF)\n",
    "fittedmaScaler.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee2cb8d",
   "metadata": {},
   "source": [
    "ElementwiseProduct\n",
    "\n",
    "The ElementwiseProduct allows us to scale each value in a vector by an arbitrary value. For example, given the vector below and the row “1, 0.1, -1” the output will be “10, 1.5, -20.” Naturally the dimensions of the scaling vector must match the dimensions of the vector inside the relevant column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27947541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------+\n",
      "| id|      features|ElementwiseProduct_8ae8188520f3__output|\n",
      "+---+--------------+---------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  1|[3.0,10.1,3.0]|                      [30.0,151.5,60.0]|\n",
      "+---+--------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\n",
    "scalingUp = ElementwiseProduct()\\\n",
    ".setScalingVec(scaleUpVec)\\\n",
    ".setInputCol(\"features\")\n",
    "scalingUp.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78476027",
   "metadata": {},
   "source": [
    "Normalizer\n",
    "\n",
    "The normalizer allows us to scale multidimensional vectors using one of several power norms, set through the parameter “p”. For example, we can use the Manhattan norm (or Manhattan distance) with p = 1, Euclidean norm with p = 2, and so on. The Manhattan distance is a measure of distance where you can only travel from point to point along the straight lines of an axis (like the streets in Manhattan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb77e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------+\n",
      "| id|      features|Normalizer_9bd8b7d55ef0__output|\n",
      "+---+--------------+-------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  1|[3.0,10.1,3.0]|           [0.18633540372670...|\n",
      "+---+--------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "manhattanDistance = Normalizer().setP(1).setInputCol(\"features\")\n",
    "manhattanDistance.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68422a4",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "    Working with Categorical Features\n",
    "-----------------------------------------\n",
    "\n",
    "The most common task for categorical features is indexing. \n",
    "\n",
    "Indexing converts a categorical variable in a column to a numerical one that you can plug into machine learning algorithms. While this is conceptually simple, there are some catches that are important to keep in mind so that Spark can do this in a stable and repeatable manner.\n",
    "\n",
    "In general, we recommend re-indexing every categorical variable when pre-processing just for consistency’s sake. This can be helpful in maintaining your models over the long run as your encoding practices may change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455edb31",
   "metadata": {},
   "source": [
    "StringIndexer\n",
    "\n",
    "The simplest way to index is via the StringIndexer, which maps strings to different numerical IDs. Spark’s StringIndexer also creates metadata attached to the DataFrame that specify what inputs correspond to what outputs. This allows us later to get inputs back from their respective index values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec230c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "lblIndxr = StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639bd78a",
   "metadata": {},
   "source": [
    "We can also apply StringIndexer to columns that are not strings, in which case, they will be converted to strings before being indexed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67b57f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|valueInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     0.0|\n",
      "| blue| bad|     8|14.386294994851129|     7.0|\n",
      "| blue| bad|    12|14.386294994851129|     1.0|\n",
      "|green|good|    15| 38.97187133755819|     3.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valIndexer = StringIndexer().setInputCol(\"value1\").setOutputCol(\"valueInd\")\n",
    "valIndexer.fit(simpleDF).transform(simpleDF).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e469e9",
   "metadata": {},
   "source": [
    "Keep in mind that the StringIndexer is an estimator that must be fit on the input data. This means it must see all inputs to select a mapping of inputs to IDs. If you train a StringIndexer on inputs “a,” “b,” and “c” and then go to use it against input “d,” it will throw an error by default. Another option is to skip the entire row if the input value was not a value seen during training. Going along with the previous example, an input value of “d” would cause that row to be skipped entirely. We can set this option before or after training the indexer or pipeline. More options may be added to this feature in the future but as of Spark 2.2, you can only skip or throw an error on invalid inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36cd8e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringIndexerModel: uid=StringIndexer_bd7db9073a44, handleInvalid=skip"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valIndexer.setHandleInvalid(\"skip\")\n",
    "valIndexer.fit(simpleDF).setHandleInvalid(\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2bd81d",
   "metadata": {},
   "source": [
    "Converting Indexed Values Back to Text\n",
    "\n",
    "When inspecting your machine learning results, you’re likely going to want to map back to the original values. Since MLlib classification models make predictions using the indexed values, this conversion is useful for converting model predictions (indices) back to the original categories. We can do this with IndexToString. You’ll notice that we do not have to input our value to the String key; Spark’s MLlib maintains this metadata for you. You can optionally specify the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc0b8940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "|color| lab|value1|            value2|labelInd|IndexToString_ba8447cc3632__output|\n",
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "|green|good|     1|14.386294994851129|     1.0|                              good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|                               bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|                               bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|                              good|\n",
      "|green|good|    12|14.386294994851129|     1.0|                              good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|                               bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|                              good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|                               bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|                               bad|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|                               bad|\n",
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "labelReverse = IndexToString().setInputCol(\"labelInd\")\n",
    "labelReverse.transform(idxRes).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d6840",
   "metadata": {},
   "source": [
    "Indexing in Vectors\n",
    "\n",
    "VectorIndexer is a helpful tool for working with categorical variables that are already found inside of vectors in your dataset. This tool will automatically find categorical features inside of your input vectors and convert them to categorical features with zero-based category indices. \n",
    "\n",
    "For example, in the following DataFrame, the first column in our Vector is a categorical variable with two different categories while the rest of the variables are continuous. By setting maxCategories to 2 in our VectorIndexer, we are instructing Spark to take any column in our vector with two or less distinct values and convert it to a categorical variable. This can be helpful when you know how many unique values there are in your largest category because you can specify this and it will automatically index the values accordingly. Conversely, Spark changes the data based on this parameter, so if you have continuous variables that don’t appear particularly continuous (lots of repeated values) these can be unintentionally converted to categorical variables if there are too few unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8eba85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+\n",
      "|     features|label|        idxed|\n",
      "+-------------+-----+-------------+\n",
      "|[1.0,2.0,3.0]|    1|[0.0,2.0,3.0]|\n",
      "|[2.0,5.0,6.0]|    2|[1.0,5.0,6.0]|\n",
      "|[1.0,8.0,9.0]|    3|[0.0,8.0,9.0]|\n",
      "+-------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "idxIn = spark.createDataFrame([\n",
    "(Vectors.dense(1, 2, 3),1),\n",
    "(Vectors.dense(2, 5, 6),2),\n",
    "(Vectors.dense(1, 8, 9),3)\n",
    "]).toDF(\"features\", \"label\")\n",
    "\n",
    "indxr = VectorIndexer()\\\n",
    "    .setInputCol(\"features\")\\\n",
    "    .setOutputCol(\"idxed\")\\\n",
    "    .setMaxCategories(2)\n",
    "\n",
    "indxr.fit(idxIn).transform(idxIn).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4b0d9",
   "metadata": {},
   "source": [
    "One-Hot Encoding\n",
    "\n",
    "Indexing categorical variables is only half of the story. One-hot encoding is an extremely common data transformation performed after indexing categorical variables. This is because indexing does not always represent our categorical variables in the correct way for downstream models to process. For instance, when we index our “color” column, you will notice that some colors have a higher value (or index number) than others (in our case, blue is 1 and green is 2). \n",
    "\n",
    "This is incorrect because it gives the mathematical appearance that the input to the machine learning algorithm seems to specify that green > blue, which makes no sense in the case of the current categories. To avoid this, we use OneHotEncoder, which will convert each distinct value to a Boolean flag (1 or 0) as a component in a vector. When we encode the color value, then we can see these are no longer ordered, making them easier for downstream models (e.g., a linear model) to process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33aa5987",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OneHotEncoder' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30/495168839.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcolorLab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlblIndxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimpleDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimpleDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"color\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mohe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"colorInd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolorLab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'OneHotEncoder' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "lblIndxr = StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "ohe = OneHotEncoder().setInputCol(\"colorInd\")\n",
    "ohe.transform(colorLab).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cb594",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "    Text Data Transformers\n",
    "-----------------------------\n",
    "\n",
    "Text is always tricky input because it often requires lots of manipulation to map to a format that a machine learning model will be able to use effectively. \n",
    "\n",
    "There are generally two kinds of texts you’ll see: free-form text and string categorical variables. This section primarily focuses on free form text because we already discussed categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae98af6",
   "metadata": {},
   "source": [
    "Tokenizing Text\n",
    "\n",
    "Tokenization is the process of converting free-form text into a list of “tokens” or individual words. The easiest way to do this is by using the Tokenizer class. This transformer will take a string of words, separated by whitespace, and convert them into an array of words. For example, in our dataset we might want to convert the Description field into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5691619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, , water, transfer, tattoos]      |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, , doorstop, red]         |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "tokenized.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad68a0e",
   "metadata": {},
   "source": [
    "We can also create a Tokenizer that is not just based white space but a regular expression with the RegexTokenizer. The format of the regular expression should conform to the Java Regular Expression (RegEx) syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b8a1c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, water, transfer, tattoos]        |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, doorstop, red]           |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "rt = RegexTokenizer()\\\n",
    "    .setInputCol(\"Description\")\\\n",
    "    .setOutputCol(\"DescOut\")\\\n",
    "    .setPattern(\" \")\\\n",
    "    .setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1c75c",
   "metadata": {},
   "source": [
    "Another way of using the RegexTokenizer is to use it to output values matching the provided pattern instead of using it as a gap. We do this by setting the gaps parameter to false. \n",
    "\n",
    "Doing this with a space as a pattern returns all the spaces, which is not too useful, but if we made our pattern capture individual words, we could return those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ccb960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------+\n",
      "|Description                        |DescOut           |\n",
      "+-----------------------------------+------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[ ,  ]            |\n",
      "|DOUGHNUT LIP GLOSS                 |[ ,  ,  ]         |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[ ,  ,  ,  ]      |\n",
      "|BLUE HARMONICA IN BOX              |[ ,  ,  ,  ]      |\n",
      "|GUMBALL COAT RACK                  |[ ,  ]            |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[ ,  ,  ,  ,  ]   |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[ ,  ,  ]         |\n",
      "|CAMOUFLAGE LED TORCH               |[ ,  ]            |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[ ,  ,  ,  ,  ]   |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[ ,  ,  ,  ]      |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[ ,  ,  ,  ]      |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[ ,  ,  ,  ]      |\n",
      "|ROSE CARAVAN DOORSTOP              |[ ,  ]            |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[ ,  ,  ,  ]      |\n",
      "|STORAGE TIN VINTAGE LEAF           |[ ,  ,  ]         |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[ ,  ,  ,  ,  ,  ]|\n",
      "|POPCORN HOLDER                     |[ ]               |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[ ,  ,  ,  ,  ,  ]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[ ,  ,  ,  ,  ]   |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[ ,  ,  ,  ,  ]   |\n",
      "+-----------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "rt = RegexTokenizer()\\\n",
    "    .setInputCol(\"Description\")\\\n",
    "    .setOutputCol(\"DescOut\")\\\n",
    "    .setPattern(\" \")\\\n",
    "    .setGaps(False)\\\n",
    "    .setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e97586",
   "metadata": {},
   "source": [
    "Removing Common Words\n",
    "\n",
    "A common task after tokenization is to filter stop words, common words that are not relevant in many kinds of analysis and should thus be removed. Frequently occurring stop words in English include “the,” “and,” and “but.” \n",
    "\n",
    "Spark contains a list of default stop words you can see by calling the following method, which can be made case insensitive if necessary (as of Spark 2.2, supported languages for stopwords are “danish,” “dutch,” “english,” “finnish,” “french,” “german,” “hungarian,” “italian,” “norwegian,” “portuguese,” “russian,” “spanish,” “swedish,” and “turkish”):\n",
    "\n",
    "Notice how the word of is removed in the output column. That’s because it’s such a common word that it isn’t relevant to any downstream manipulation and simply adds noise to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14d85def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------------------------+\n",
      "|         Description|             DescOut|StopWordsRemover_c9750c35a1dc__output|\n",
      "+--------------------+--------------------+-------------------------------------+\n",
      "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|                 [rabbit, night, l...|\n",
      "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|                 [doughnut, lip, g...|\n",
      "|12 MESSAGE CARDS ...|[12, message, car...|                 [12, message, car...|\n",
      "|BLUE HARMONICA IN...|[blue, harmonica,...|                 [blue, harmonica,...|\n",
      "|   GUMBALL COAT RACK|[gumball, coat, r...|                 [gumball, coat, r...|\n",
      "|SKULLS  WATER TRA...|[skulls, , water,...|                 [skulls, , water,...|\n",
      "|FELTCRAFT GIRL AM...|[feltcraft, girl,...|                 [feltcraft, girl,...|\n",
      "|CAMOUFLAGE LED TORCH|[camouflage, led,...|                 [camouflage, led,...|\n",
      "|WHITE SKULL HOT W...|[white, skull, ho...|                 [white, skull, ho...|\n",
      "|ENGLISH ROSE HOT ...|[english, rose, h...|                 [english, rose, h...|\n",
      "|HOT WATER BOTTLE ...|[hot, water, bott...|                 [hot, water, bott...|\n",
      "|SCOTTIE DOG HOT W...|[scottie, dog, ho...|                 [scottie, dog, ho...|\n",
      "|ROSE CARAVAN DOOR...|[rose, caravan, d...|                 [rose, caravan, d...|\n",
      "|GINGHAM HEART  DO...|[gingham, heart, ...|                 [gingham, heart, ...|\n",
      "|STORAGE TIN VINTA...|[storage, tin, vi...|                 [storage, tin, vi...|\n",
      "|SET OF 4 KNICK KN...|[set, of, 4, knic...|                 [set, 4, knick, k...|\n",
      "|      POPCORN HOLDER|   [popcorn, holder]|                    [popcorn, holder]|\n",
      "|GROW A FLYTRAP OR...|[grow, a, flytrap...|                 [grow, flytrap, s...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|                 [airline, bag, vi...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|                 [airline, bag, vi...|\n",
      "+--------------------+--------------------+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    "    .setStopWords(englishStopWords)\\\n",
    "    .setInputCol(\"DescOut\")\n",
    "stops.transform(tokenized).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a51cde",
   "metadata": {},
   "source": [
    "Creating Word Combinations\n",
    "\n",
    "Tokenizing our strings and filtering stop words leaves us with a clean set of words to use as features. It is often of interest to look at combinations of words, usually by looking at colocated words. Word combinations are technically referred to as n-grams—that is, sequences of words of length n. An n-gram of length 1 is called a unigrams; those of length 2 are called bigrams, and those of length 3 are called trigrams (anything above those are just four-gram, five-gram, etc.), Order matters with n-gram creation, so converting a sentence with three words into bigram representation would result in two bigrams. The goal when creating n-grams is to better capture sentence structure and more information than can be gleaned by simply looking at all words individually. Let’s create some n-grams to illustrate this concept.\n",
    "\n",
    "The bigrams of “Big Data Processing Made Simple” are:\n",
    "    “Big Data”\n",
    "    “Data Processing”\n",
    "    “Processing Made”\n",
    "    “Made Simple”\n",
    "\n",
    "While the trigrams are:\n",
    "    “Big Data Processing”\n",
    "    “Data Processing Made”\n",
    "    “Procesing Made Simple”\n",
    "\n",
    "With n-grams, we can look at sequences of words that commonly co-occur and use them as inputs to a machine learning algorithm. These can create better features than simply looking at all of the words individually (say, tokenized on a space character):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b6edb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+\n",
      "|             DescOut|NGram_28261e397a96__output|\n",
      "+--------------------+--------------------------+\n",
      "|[rabbit, night, l...|      [rabbit, night, l...|\n",
      "|[doughnut, lip, g...|      [doughnut, lip, g...|\n",
      "|[12, message, car...|      [12, message, car...|\n",
      "|[blue, harmonica,...|      [blue, harmonica,...|\n",
      "|[gumball, coat, r...|      [gumball, coat, r...|\n",
      "|[skulls, , water,...|      [skulls, , water,...|\n",
      "|[feltcraft, girl,...|      [feltcraft, girl,...|\n",
      "|[camouflage, led,...|      [camouflage, led,...|\n",
      "|[white, skull, ho...|      [white, skull, ho...|\n",
      "|[english, rose, h...|      [english, rose, h...|\n",
      "+--------------------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+--------------------------+\n",
      "|             DescOut|NGram_215d6b0e1c05__output|\n",
      "+--------------------+--------------------------+\n",
      "|[rabbit, night, l...|      [rabbit night, ni...|\n",
      "|[doughnut, lip, g...|      [doughnut lip, li...|\n",
      "|[12, message, car...|      [12 message, mess...|\n",
      "|[blue, harmonica,...|      [blue harmonica, ...|\n",
      "|[gumball, coat, r...|      [gumball coat, co...|\n",
      "|[skulls, , water,...|      [skulls ,  water,...|\n",
      "|[feltcraft, girl,...|      [feltcraft girl, ...|\n",
      "|[camouflage, led,...|      [camouflage led, ...|\n",
      "|[white, skull, ho...|      [white skull, sku...|\n",
      "|[english, rose, h...|      [english rose, ro...|\n",
      "+--------------------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "unigram = NGram().setInputCol(\"DescOut\").setN(1)\n",
    "bigram = NGram().setInputCol(\"DescOut\").setN(2)\n",
    "unigram.transform(tokenized.select(\"DescOut\")).show(10)\n",
    "bigram.transform(tokenized.select(\"DescOut\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de3f5a",
   "metadata": {},
   "source": [
    "Converting Words into Numerical Representations\n",
    "\n",
    "Once you have word features, it’s time to start counting instances of words and word combinations for use in our models. The simplest way is just to include binary counts of a word in a given document (in our case, a row). Essentially, we’re measuring whether or not each row contains a given word. This is a simple way to normalize for document sizes and occurrence counts and get numerical features that allow us to classify documents based on content. In addition, we can count words using a CountVectorizer, or reweigh them according to the prevalence of a given word in all the documents using a TF–IDF transformation (discussed next).\n",
    "\n",
    "A CountVectorizer operates on our tokenized data and does two things:\n",
    "\n",
    "1. During the fit process, it finds the set of words in all the documents and then counts\n",
    "the occurrences of those words in those documents.\n",
    "\n",
    "2. It then counts the occurrences of a given word in each row of the DataFrame column\n",
    "during the transformation process and outputs a vector with the terms that occur in that\n",
    "row.\n",
    "\n",
    "Conceptually this tranformer treats every row as a document and every word as a term and the\n",
    "total collection of all terms as the vocabulary. These are all tunable parameters, meaning we can set the minimum term frequency (minTF) for the term to be included in the vocabulary\n",
    "(effectively removing rare words from the vocabulary); minimum number of documents a term\n",
    "must appear in (minDF) before being included in the vocabulary (another way to remove rare\n",
    "words from the vocabulary); and finally, the total maximum vocabulary size (vocabSize).\n",
    "Lastly, by default the CountVectorizer will output the counts of a term in a document. To just return whether or not a word exists in a document, we can use setBinary(true). Here’s an example of using CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd2c0e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|             DescOut|            countVec|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|(500,[149,185,212...|\n",
      "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|(500,[462,463,492...|\n",
      "|12 MESSAGE CARDS ...|[12, message, car...|(500,[35,41,166],...|\n",
      "|BLUE HARMONICA IN...|[blue, harmonica,...|(500,[10,16,36,35...|\n",
      "|   GUMBALL COAT RACK|[gumball, coat, r...|(500,[228,281,408...|\n",
      "|SKULLS  WATER TRA...|[skulls, , water,...|(500,[11,40,133],...|\n",
      "|FELTCRAFT GIRL AM...|[feltcraft, girl,...|(500,[60,64,69],[...|\n",
      "|CAMOUFLAGE LED TORCH|[camouflage, led,...|   (500,[264],[1.0])|\n",
      "|WHITE SKULL HOT W...|[white, skull, ho...|(500,[15,34,39,40...|\n",
      "|ENGLISH ROSE HOT ...|[english, rose, h...|(500,[34,39,40,46...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer()\\\n",
    "    .setInputCol(\"DescOut\")\\\n",
    "    .setOutputCol(\"countVec\")\\\n",
    "    .setVocabSize(500)\\\n",
    "    .setMinTF(1)\\\n",
    "    .setMinDF(2)\n",
    "fittedCV = cv.fit(tokenized)\n",
    "fittedCV.transform(tokenized).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17577e38",
   "metadata": {},
   "source": [
    "Term frequency–inverse document frequency\n",
    "\n",
    "Another way to approach the problem of converting text into a numerical representation is to use term frequency–inverse document frequency (TF–IDF). In simplest terms, TF–IDF measures how often a word occurs in each document, weighted according to how many documents that word occurs in. The result is that words that occur in a few documents are given more weight than words that occur in many documents. In practice, a word like “the” would be weighted very low because of its prevalence while a more specialized word like “streaming” would occur in fewer documents and thus would be weighted higher. In a way, TF–IDF helps find documents that share similar topics. Let’s take a look at an example—first, we’ll inspect some of the documents in our data containing the word “red”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a64c919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|DescOut                                |\n",
      "+---------------------------------------+\n",
      "|[gingham, heart, , doorstop, red]      |\n",
      "|[red, floral, feltcraft, shoulder, bag]|\n",
      "|[alarm, clock, bakelike, red]          |\n",
      "|[pin, cushion, babushka, red]          |\n",
      "|[red, retrospot, mini, cases]          |\n",
      "|[red, kitchen, scales]                 |\n",
      "|[gingham, heart, , doorstop, red]      |\n",
      "|[large, red, babushka, notebook]       |\n",
      "|[red, retrospot, oven, glove]          |\n",
      "|[red, retrospot, plate]                |\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfIdfIn = tokenized\\\n",
    "    .where(\"array_contains(DescOut, 'red')\")\\\n",
    "    .select(\"DescOut\")\\\n",
    "    .limit(10)\n",
    "tfIdfIn.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43f930",
   "metadata": {},
   "source": [
    "We can see some overlapping words in these documents, but these words provide at least a rough topic-like representation. Now let’s input that into TF–IDF. To do this, we’re going to hash each word and convert it to a numerical representation, and then weigh each word in the voculary according to the inverse document frequency. Hashing is a similar process as CountVectorizer, but is irreversible—that is, from our output index for a word, we cannot get our input word (multiple words might map to the same output index):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1bc44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "tf = HashingTF()\\\n",
    "    .setInputCol(\"DescOut\")\\\n",
    "    .setOutputCol(\"TFOut\")\\\n",
    "    .setNumFeatures(10000)\n",
    "idf = IDF()\\\n",
    "    .setInputCol(\"TFOut\")\\\n",
    "    .setOutputCol(\"IDFOut\")\\\n",
    "    .setMinDocFreq(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a19eabc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|DescOut                                |TFOut                                                |IDFOut                                                                                                           |\n",
      "+---------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[52,804,3372,6594,9808],[1.0,1.0,1.0,1.0,1.0])|(10000,[52,804,3372,6594,9808],[0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[red, floral, feltcraft, shoulder, bag]|(10000,[50,52,415,6756,8005],[1.0,1.0,1.0,1.0,1.0])  |(10000,[50,52,415,6756,8005],[0.0,0.0,0.0,0.0,0.0])                                                              |\n",
      "|[alarm, clock, bakelike, red]          |(10000,[52,4995,8737,9001],[1.0,1.0,1.0,1.0])        |(10000,[52,4995,8737,9001],[0.0,0.0,0.0,0.0])                                                                    |\n",
      "|[pin, cushion, babushka, red]          |(10000,[52,610,2490,7153],[1.0,1.0,1.0,1.0])         |(10000,[52,610,2490,7153],[0.0,0.0,0.0,1.2992829841302609])                                                      |\n",
      "|[red, retrospot, mini, cases]          |(10000,[52,547,6703,8448],[1.0,1.0,1.0,1.0])         |(10000,[52,547,6703,8448],[0.0,0.0,0.0,1.0116009116784799])                                                      |\n",
      "|[red, kitchen, scales]                 |(10000,[52,756,6452],[1.0,1.0,1.0])                  |(10000,[52,756,6452],[0.0,0.0,0.0])                                                                              |\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[52,804,3372,6594,9808],[1.0,1.0,1.0,1.0,1.0])|(10000,[52,804,3372,6594,9808],[0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[large, red, babushka, notebook]       |(10000,[52,2787,7022,7153],[1.0,1.0,1.0,1.0])        |(10000,[52,2787,7022,7153],[0.0,0.0,0.0,1.2992829841302609])                                                     |\n",
      "|[red, retrospot, oven, glove]          |(10000,[52,8242,8448,8667],[1.0,1.0,1.0,1.0])        |(10000,[52,8242,8448,8667],[0.0,0.0,1.0116009116784799,0.0])                                                     |\n",
      "|[red, retrospot, plate]                |(10000,[52,4925,8448],[1.0,1.0,1.0])                 |(10000,[52,4925,8448],[0.0,0.0,1.0116009116784799])                                                              |\n",
      "+---------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474165c",
   "metadata": {},
   "source": [
    "Word2Vec\n",
    "\n",
    "Word2Vec is a deep learning–based tool for computing a vector representation of a set of words.\n",
    "\n",
    "The goal is to have similar words close to one another in this vector space, so we can then make generalizations about the words themselves. This model is easy to train and use, and has been shown to be useful in a number of natural language processing applications, including entity recognition, disambiguation, parsing, tagging, and machine translation.\n",
    "\n",
    "Word2Vec is notable for capturing relationships between words based on their semantics. For example, if v~king, v~queen, v~man, and v~women represent the vectors for those four words, then we will often get a representation where v~king − v~man + v~woman ~= v~queen. To do this, Word2Vec uses a technique called “skip-grams” to convert a sentence of words into a vector representation (optionally of a specific size). It does this by building a vocabulary, and then for every sentence, it removes a token and trains the model to predict the missing token in the \"n-gram” representation. Word2Vec works best with continuous, free-form text in the form of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f547c3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/30 15:27:37 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/01/30 15:27:37 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.03626798838376999,0.0632577270269394,-0.04630278870463372]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.044452465272375515,-0.0005993843078613281,-0.0497912083353315]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.0724957874044776,0.11760260164737701,-0.02727147787809372]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\",\n",
    "outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12fb4a",
   "metadata": {},
   "source": [
    "Feature Manipulation\n",
    "\n",
    "While nearly every transformer in ML manipulates the feature space in some way, the following algorithms and tools are automated means of either expanding the input feature vectors or reducing them to a lower number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04be2218",
   "metadata": {},
   "source": [
    "PCA\n",
    "\n",
    "Principal Components Analysis (PCA) is a mathematical technique for finding the most important aspects of our data (the principal components). It changes the feature representation of our data by creating a new set of features (“aspects”). Each new feature is a combination of the original features. The power of PCA is that it can create a smaller set of more meaningful features to be input into your model, at the potential cost of interpretability.\n",
    "\n",
    "You’d want to use PCA if you have a large input dataset and want to reduce the total number of features you have. This frequently comes up in text analysis where the entire feature space is massive and many of the features are largely irrelevant. Using PCA, we can find the most important combinations of features and only include those in our machine learning model. PCA takes a parameter ὅ, specifying the number of output features to create. Generally, this should be much smaller than your input vectors’ dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a1b3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/30 15:29:08 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "22/01/30 15:29:08 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------+\n",
      "|id |features      |PCA_135e6259512e__output                  |\n",
      "+---+--------------+------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n",
      "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060155864]|\n",
      "+---+--------------+------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA().setInputCol(\"features\").setK(2)\n",
    "pca.fit(scaleDF).transform(scaleDF).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aade4abc",
   "metadata": {},
   "source": [
    "Interaction\n",
    "\n",
    "In some cases, you might have domain knowledge about specific variables in your dataset. For example, you might know that a certain interaction between the two variables is an important variable to include in a downstream estimator. The feature transformer Interaction allows you to create an interaction between two variables manually. It just multiplies the two features together—something that a typical linear model would not do for every possible pair of features in your data. This transformer is currently only available directly in Scala but can be called from any language using the RFormula. We recommend users just use RFormula instead of manually\n",
    "creating interactions.\n",
    "\n",
    "Polynomial Expansion\n",
    "\n",
    "Polynomial expansion is used to generate interaction variables of all the input columns. With polynomial expansion, we specify to what degree we would like to see various interactions. For example, for a degree-2 polynomial, Spark takes every value in our feature vector, multiplies it by every other value in the feature vector, and then stores the results as features. For instance, if we have two input features, we’ll get four output features if we use a second degree polynomial (2x2). If we have three input features, we’ll get nine output features (3x3). If we use a third degree polynomial, we’ll get 27 output features (3x3x3) and so on. This transformation is useful when you want to see interactions between particular features but aren’t necessarily sure about which interactions to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d17f0994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+----------------------------------------+\n",
      "| id|      features|PolynomialExpansion_a55cba461f16__output|\n",
      "+---+--------------+----------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                    [1.0,1.0,0.1,0.1,...|\n",
      "|  1| [2.0,1.1,1.0]|                    [2.0,4.0,1.1,2.2,...|\n",
      "|  0|[1.0,0.1,-1.0]|                    [1.0,1.0,0.1,0.1,...|\n",
      "|  1| [2.0,1.1,1.0]|                    [2.0,4.0,1.1,2.2,...|\n",
      "|  1|[3.0,10.1,3.0]|                    [3.0,9.0,10.1,30....|\n",
      "+---+--------------+----------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "pe = PolynomialExpansion().setInputCol(\"features\").setDegree(2)\n",
    "pe.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e052323",
   "metadata": {},
   "source": [
    "Feature Selection\n",
    "\n",
    "Often, you will have a large range of possible features and want to select a smaller subset to use for training. For example, many features might be correlated, or using too many features might lead to overfitting. This process is called feature selection. There are a number of ways to evaluate feature importance once you’ve trained a model but another option is to do some rough filtering beforehand. Spark has some simple options for doing that, such as ChiSqSelector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d8774c",
   "metadata": {},
   "source": [
    "ChiSqSelector\n",
    "\n",
    "ChiSqSelector leverages a statistical test to identify features that are not independent from the label we are trying to predict, and drop the uncorrelated features. It’s often used with categorical data in order to reduce the number of features you will input into your model, as well as to reduce the dimensionality of text data (in the form of frequencies or counts). Since this method is based on the Chi-Square test, there are several different ways we can pick the “best” features.\n",
    "\n",
    "The methods are numTopFeatures, which is ordered by p-value; percentile, which takes a proportion of the input features (instead of just the top N features); and fpr, which sets a cut off p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0643fea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------------------+\n",
      "|            countVec|ChiSqSelector_95fd196f4c66__output|\n",
      "+--------------------+----------------------------------+\n",
      "|(500,[149,185,212...|                         (2,[],[])|\n",
      "|(500,[462,463,492...|                         (2,[],[])|\n",
      "|(500,[35,41,166],...|                         (2,[],[])|\n",
      "|(500,[10,16,36,35...|                         (2,[],[])|\n",
      "|(500,[228,281,408...|                         (2,[],[])|\n",
      "|(500,[11,40,133],...|                         (2,[],[])|\n",
      "|(500,[60,64,69],[...|                         (2,[],[])|\n",
      "|   (500,[264],[1.0])|                         (2,[],[])|\n",
      "|(500,[15,34,39,40...|                         (2,[],[])|\n",
      "|(500,[34,39,40,46...|                         (2,[],[])|\n",
      "|(500,[34,39,40,14...|                         (2,[],[])|\n",
      "|(500,[34,39,40,14...|                         (2,[],[])|\n",
      "|(500,[46,297],[1....|                         (2,[],[])|\n",
      "|(500,[3,4,11,143,...|                         (2,[],[])|\n",
      "|(500,[6,45,109,16...|                         (2,[],[])|\n",
      "|(500,[0,1,49,70,3...|               (2,[0,1],[1.0,1.0])|\n",
      "|(500,[21,296],[1....|                         (2,[],[])|\n",
      "|(500,[36,45,378],...|                         (2,[],[])|\n",
      "|(500,[2,6,328],[1...|                         (2,[],[])|\n",
      "|(500,[0,2,6,328,4...|                     (2,[0],[1.0])|\n",
      "+--------------------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector, Tokenizer\n",
    "\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "\n",
    "tokenized = tkn\\\n",
    "    .transform(sales.select(\"Description\", \"CustomerId\"))\\\n",
    "    .where(\"CustomerId IS NOT NULL\")\n",
    "\n",
    "prechi = fittedCV.transform(tokenized)\\\n",
    "    .where(\"CustomerId IS NOT NULL\")\n",
    "\n",
    "chisq = ChiSqSelector()\\\n",
    "    .setFeaturesCol(\"countVec\")\\\n",
    "    .setLabelCol(\"CustomerId\")\\\n",
    "    .setNumTopFeatures(2)\n",
    "\n",
    "chisq.fit(prechi).transform(prechi)\\\n",
    "    .drop(\"customerId\", \"Description\", \"DescOut\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c41530",
   "metadata": {},
   "source": [
    "Advanced Topics\n",
    "\n",
    "There are several advanced topics surrounding transformers and estimators. Here we touch on the two most common, persisting transformers as well as writing custom ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e775d5",
   "metadata": {},
   "source": [
    "Persisting Transformers\n",
    "\n",
    "Once you’ve used an estimator to configure a transformer, it can be helpful to write it to disk and simply load it when necessary (e.g., for use in another Spark session). We saw this in the previous chapter when we persisted an entire pipeline. To persist a transformer individually, we use the write method on the fitted transformer (or the standard transformer) and specify the location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6e528d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedPCA = pca.fit(scaleDF)\n",
    "fittedPCA.write().overwrite().save(\"/tmp/fittedPCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a8110e",
   "metadata": {},
   "source": [
    "We can then load it back in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45eeea7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------+\n",
      "| id|      features|PCAModel_6a902b41e783__output|\n",
      "+---+--------------+-----------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|         [0.07137194992484...|\n",
      "|  1| [2.0,1.1,1.0]|         [-1.6804946984073...|\n",
      "|  0|[1.0,0.1,-1.0]|         [0.07137194992484...|\n",
      "|  1| [2.0,1.1,1.0]|         [-1.6804946984073...|\n",
      "|  1|[3.0,10.1,3.0]|         [-10.872398139848...|\n",
      "+---+--------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCAModel\n",
    "loadedPCA = PCAModel.load(\"/tmp/fittedPCA\")\n",
    "loadedPCA.transform(scaleDF).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
