{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e585834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/27 05:56:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/01/27 05:56:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").\\\n",
    "                                     appName(\"spark_on_docker\").\\\n",
    "                                     getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f8026a",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "    Formatting Models According to Your Use Case\n",
    "----------------------------------------------------\n",
    "\n",
    "To preprocess data for Spark’s different advanced analytics tools, you must consider your end objective. The following list walks through the requirements for input data structure for each advanced analytics task in MLlib:\n",
    "\n",
    "    - In the case of most classification and regression algorithms, you want to get your data into a column of type Double to represent the label and a column of type Vector (either dense or sparse) to represent the features.\n",
    "\n",
    "    - In the case of recommendation, you want to get your data into a column of users, a column of items (say movies or books), and a column of ratings.\n",
    "\n",
    "    - In the case of unsupervised learning, a column of type Vector (either dense or sparse) is needed to represent the features.\n",
    "\n",
    "    - In the case of graph analytics, you will want a DataFrame of vertices and a DataFrame of edges.\n",
    "\n",
    "\n",
    "The best way to get your data in these formats is through transformers. Transformers are functions that accept a DataFrame as an argument and return a new DataFrame as a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11bbaf",
   "metadata": {},
   "source": [
    "Before we proceed, we’re going to read in several different sample datasets, each of which has\n",
    "different properties we will manipulate in this chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc5a73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "sales = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\")\\\n",
    "    .coalesce(5)\\\n",
    "    .where(\"Description IS NOT NULL\")\n",
    "fakeIntDF = spark.read.parquet(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/simple-ml-integers\")\n",
    "simpleDF = spark.read.json(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/simple-ml\")\n",
    "scaleDF = spark.read.parquet(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/simple-ml-scaling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167446dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales.cache()\n",
    "sales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79e332",
   "metadata": {},
   "source": [
    "--------------------\n",
    "    Transformers\n",
    "--------------------\n",
    "\n",
    "Transformers are functions that convert raw data in some way. This might be to create a new interaction variable (from two other variables), to normalize a column, or to simply turn it into a Double to be input into a model. Transformers are primarily used in preprocessing or feature generation.\n",
    "\n",
    "Spark’s transformer only includes a transform method. This is because it will not change based on the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cae211",
   "metadata": {},
   "source": [
    "Estimators for Preprocessing\n",
    "\n",
    "An estimator is necessary when a transformation you would like to perform must be initialized with data or information about the input column (often derived by doing a pass over the input column itself). \n",
    "\n",
    "For example, if you wanted to scale the values in our column to have mean zero and unit variance, you would need to perform a pass over the entire data in order to calculate the values you would use to normalize the data to mean zero and unit variance. In effect, an estimator can be a transformer configured according to your particular input data. In simplest terms, you can either blindly apply a transformation (a “regular” transformer type) or perform a transformation based on your data (an estimator type).\n",
    "\n",
    "An example of this type of estimator is the StandardScaler, which scales your input column according to the range of values in that column to have a zero mean and a variance of 1 in each dimension. For that reason it must first perform a pass over the data to create the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02ab39",
   "metadata": {},
   "source": [
    "High-Level Transformers\n",
    "\n",
    "High-level transformers allow you to concisely specify a number of transformations in one. These operate at a “high level”, and allow you to avoid doing data manipulations or transformations one by one. \n",
    "\n",
    "In general, you should try to use the highest level transformers you can, in order to minimize the risk of error and help you focus on the business problem instead of the smaller details of implementation. While this is not always possible, it’s a good objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887a3d4",
   "metadata": {},
   "source": [
    "RFormula\n",
    "\n",
    "The RFormula is the easiest transfomer to use when you have “conventionally” formatted data. \n",
    "\n",
    "Spark borrows this transformer from the R language to make it simple to declaratively specify a set of transformations for your data.\n",
    "\n",
    "The RFormula will automatically handle categorical inputs (specified as strings) by performing something called one-hot encoding. \n",
    "\n",
    "With the RFormula, numeric columns will be cast to Double but will not be one-hot encoded. If the label column is of type String, it will be first transformed to Double with StringIndexer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea63d20",
   "metadata": {},
   "source": [
    "The RFormula allows you to specify your transformations in declarative syntax. It is simple to use once you understand the syntax. Currently, RFormula supports a limited subset of the R operators that in practice work quite well for simple transformations. The basic operators are:\n",
    "    \n",
    "    ~\n",
    "    Separate target and terms\n",
    "\n",
    "    +\n",
    "    Concatenate terms; “+ 0” means removing the intercept (this means the y-intercept of the line\n",
    "    that we will fit will be 0)\n",
    "    \n",
    "    -\n",
    "    Remove a term; “- 1” means removing intercept (this means the y-intercept of the line that\n",
    "    we will fit will be 0)\n",
    "    \n",
    "    :\n",
    "    Interaction (multiplication for numeric values, or binarized categorical values)\n",
    "    \n",
    "    .\n",
    "    All columns except the target/dependent variabl\n",
    "\n",
    "RFormula also uses default columns of label and features to label, you guessed it, the label and the set of features that it outputs (for supervised machine learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05844885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
      "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\n",
    "supervised.fit(simpleDF).transform(simpleDF).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf4829",
   "metadata": {},
   "source": [
    "SQL Transformers\n",
    "\n",
    "A SQLTransformer allows you to leverage Spark’s vast library of SQL-related manipulations just as you would a MLlib transformation. \n",
    "\n",
    "Any SELECT statement you can use in SQL is a valid transformation. The only thing you need to change is that instead of using the table name, you should just use the keyword THIS. \n",
    "\n",
    "You might want to use SQLTransformer if you want to formally codify some DataFrame manipulation as a preprocessing step, or try different SQL expressions for features during hyperparameter tuning. Also note that the output of this transformation will be appended as a column to the output DataFrame.\n",
    "\n",
    "You might want to use an SQLTransformer in order to represent all of your manipulations on the very rawest form of your data so you can version different variations of manipulations as transformers. This gives you the benefit of building and testing varying pipelines, all by simply swapping out transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a4f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|         1721|     119|   18180.0|\n",
      "|         1070|     107|   12782.0|\n",
      "|          701|      59|   17402.0|\n",
      "|          478|      35|   16642.0|\n",
      "|          477|      28|   16811.0|\n",
      "|          986|      71|   15053.0|\n",
      "|         1419|      50|   12913.0|\n",
      "|          445|      43|   12628.0|\n",
      "|         4505|     183|   14401.0|\n",
      "|          271|      20|   16851.0|\n",
      "+-------------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "basicTransformation = SQLTransformer()\\\n",
    "    .setStatement(\"\"\"\n",
    "                    SELECT sum(Quantity), count(*), CustomerID\n",
    "                    FROM __THIS__\n",
    "                    GROUP BY CustomerID\n",
    "    \"\"\")\n",
    "\n",
    "basicTransformation.transform(sales).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b708e",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "    VectorAssembler\n",
    "-----------------------\n",
    "\n",
    "The VectorAssembler is a tool you’ll use in nearly every single pipeline you generate. It helps concatenate all your features into one big vector you can then pass into an estimator. \n",
    "\n",
    "It’s used typically in the last step of a machine learning pipeline and takes as input a number of columns of Boolean, Double, or Vector. This is particularly helpful if you’re going to perform a number of manipulations using a variety of transformers and need to gather all of those results together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7510d2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_6c6d56b8b957__output|\n",
      "+----+----+----+------------------------------------+\n",
      "|   1|   2|   3|                       [1.0,2.0,3.0]|\n",
      "|   4|   5|   6|                       [4.0,5.0,6.0]|\n",
      "|   7|   8|   9|                       [7.0,8.0,9.0]|\n",
      "+----+----+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d3141c",
   "metadata": {},
   "source": [
    "Working with Continuous Features\n",
    "\n",
    "Continuous features are just values on the number line, from positive infinity to negative infinity.\n",
    "\n",
    "There are two common transformers for continuous features. \n",
    "\n",
    "First, you can convert continuous features into categorical features via a process called bucketing, \n",
    "or you can scale and normalize your features according to several different requirements. \n",
    "\n",
    "These transformers will only work on Double types, so make sure you’ve turned any other numerical values to Double:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef5510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d8e3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|0.0|\n",
      "|1.0|\n",
      "|2.0|\n",
      "|3.0|\n",
      "|4.0|\n",
      "|5.0|\n",
      "|6.0|\n",
      "|7.0|\n",
      "|8.0|\n",
      "|9.0|\n",
      "+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a720d",
   "metadata": {},
   "source": [
    "-------------------\n",
    "    Bucketing\n",
    "-------------------\n",
    "\n",
    "The most straightforward approach to bucketing or binning is using the Bucketizer. This will split a given continuous feature into the buckets of your designation. You specify how buckets should be created via an array or list of Double values. This is useful because you may want to simplify the features in your dataset or simplify their representations for interpretation later on.\n",
    "\n",
    "For example, imagine you have a column that represents a person’s weight and you would like to predict some value based on this information. In some cases, it might be simpler to create three buckets of “overweight,” “average,” and “underweight.” \n",
    "\n",
    "To specify the bucket, set its borders. For example, setting splits to 5.0, 10.0, 250.0 on our contDF will actually fail because we don’t cover all possible input ranges. When specifying your bucket points, the values you pass into splits must satisfy three requirements:\n",
    "\n",
    "    - The minimum value in your splits array must be less than the minimum value in your DataFrame.\n",
    "    - The maximum value in your splits array must be greater than the maximum value in your DataFrame.\n",
    "    - You need to specify at a minimum three values in the splits array, which creates two buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d312ad",
   "metadata": {},
   "source": [
    "In addition to splitting based on hardcoded values, another option is to split based on percentiles in our data. This is done with QuantileDiscretizer, which will bucket the values into user\u0002specified buckets with the splits being determined by approximate quantiles values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26954dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+\n",
      "|  id|Bucketizer_76b264ea51eb__output|\n",
      "+----+-------------------------------+\n",
      "| 0.0|                            0.0|\n",
      "| 1.0|                            0.0|\n",
      "| 2.0|                            0.0|\n",
      "| 3.0|                            0.0|\n",
      "| 4.0|                            0.0|\n",
      "| 5.0|                            1.0|\n",
      "| 6.0|                            1.0|\n",
      "| 7.0|                            1.0|\n",
      "| 8.0|                            1.0|\n",
      "| 9.0|                            1.0|\n",
      "|10.0|                            2.0|\n",
      "|11.0|                            2.0|\n",
      "|12.0|                            2.0|\n",
      "|13.0|                            2.0|\n",
      "|14.0|                            2.0|\n",
      "|15.0|                            2.0|\n",
      "|16.0|                            2.0|\n",
      "|17.0|                            2.0|\n",
      "|18.0|                            2.0|\n",
      "|19.0|                            2.0|\n",
      "+----+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0af08bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------------+\n",
      "|  id|QuantileDiscretizer_d5e52fdec202__output|\n",
      "+----+----------------------------------------+\n",
      "| 0.0|                                     0.0|\n",
      "| 1.0|                                     0.0|\n",
      "| 2.0|                                     0.0|\n",
      "| 3.0|                                     1.0|\n",
      "| 4.0|                                     1.0|\n",
      "| 5.0|                                     1.0|\n",
      "| 6.0|                                     1.0|\n",
      "| 7.0|                                     2.0|\n",
      "| 8.0|                                     2.0|\n",
      "| 9.0|                                     2.0|\n",
      "|10.0|                                     2.0|\n",
      "|11.0|                                     3.0|\n",
      "|12.0|                                     3.0|\n",
      "|13.0|                                     3.0|\n",
      "|14.0|                                     3.0|\n",
      "|15.0|                                     4.0|\n",
      "|16.0|                                     4.0|\n",
      "|17.0|                                     4.0|\n",
      "|18.0|                                     4.0|\n",
      "|19.0|                                     4.0|\n",
      "+----+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "bucketer = QuantileDiscretizer().setNumBuckets(5).setInputCol(\"id\")\n",
    "fittedBucketer = bucketer.fit(contDF)\n",
    "fittedBucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6837f19",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "    Scaling and Normalization\n",
    "-----------------------------------\n",
    "\n",
    "You might want to do this when your data contains a number of columns based on different scales. \n",
    "\n",
    "For instance, say we have a DataFrame with two columns: weight (in ounces) and height (in feet). If you don’t scale or normalize, the algorithm will be less sensitive to variations in height because height values in feet are much lower than weight values in ounces. That’s an example where you should scale your data. \n",
    "\n",
    "An example of normalization might involve transforming the data so that each point’s value is a representation of its distance from the mean of that column.\n",
    "\n",
    "In MLlib, this is always done on columns of type Vector. MLlib will look across all the rows in a given column (of type Vector) and then treat every dimension in those vectors as its own particular column. It will then apply the scaling or normalization function on each dimension separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b30c53",
   "metadata": {},
   "source": [
    "----------------------\n",
    "    StandardScaler\n",
    "----------------------\n",
    "\n",
    "The StandardScaler standardizes a set of features to have zero mean and a standard deviation of 1. The flag withStd will scale the data to unit standard deviation while the flag withMean (false by default) will center the data prior to scaling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51b2a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------+\n",
      "| id|      features|StandardScaler_9ee186f65881__output|\n",
      "+---+--------------+-----------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
      "+---+--------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c912a11",
   "metadata": {},
   "source": [
    "MinMaxScaler\n",
    "\n",
    "The MinMaxScaler will scale the values in a vector (component wise) to the proportional values on a scale from a given min value to a max value. If you specify the minimum value to be 0 and the maximum value to be 1, then all the values will fall in between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "168b4f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MinMaxScaler_17b9eb1feb0e__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  1|[3.0,10.1,3.0]|                 [10.0,10.0,10.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
    "fittedminMax = minMax.fit(scaleDF)\n",
    "fittedminMax.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba4fd0",
   "metadata": {},
   "source": [
    "MaxAbsScaler\n",
    "\n",
    "The max absolute scaler (MaxAbsScaler) scales the data by dividing each value by the maximum absolute value in this feature. All values therefore end up between −1 and 1. This transformer does not shift or center the data at all in the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27398a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MaxAbsScaler_bcddc0d98dbc__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  1|[3.0,10.1,3.0]|                    [1.0,1.0,1.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "maScaler = MaxAbsScaler().setInputCol(\"features\")\n",
    "fittedmaScaler = maScaler.fit(scaleDF)\n",
    "fittedmaScaler.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee2cb8d",
   "metadata": {},
   "source": [
    "ElementwiseProduct\n",
    "\n",
    "The ElementwiseProduct allows us to scale each value in a vector by an arbitrary value. For example, given the vector below and the row “1, 0.1, -1” the output will be “10, 1.5, -20.” Naturally the dimensions of the scaling vector must match the dimensions of the vector inside the relevant column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27947541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------+\n",
      "| id|      features|ElementwiseProduct_32eb16dea613__output|\n",
      "+---+--------------+---------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  1|[3.0,10.1,3.0]|                      [30.0,151.5,60.0]|\n",
      "+---+--------------+---------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\n",
    "scalingUp = ElementwiseProduct()\\\n",
    ".setScalingVec(scaleUpVec)\\\n",
    ".setInputCol(\"features\")\n",
    "scalingUp.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78476027",
   "metadata": {},
   "source": [
    "Normalizer\n",
    "\n",
    "The normalizer allows us to scale multidimensional vectors using one of several power norms, set through the parameter “p”. For example, we can use the Manhattan norm (or Manhattan distance) with p = 1, Euclidean norm with p = 2, and so on. The Manhattan distance is a measure of distance where you can only travel from point to point along the straight lines of an axis (like the streets in Manhattan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb77e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------+\n",
      "| id|      features|Normalizer_9379351bf8c8__output|\n",
      "+---+--------------+-------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  1|[3.0,10.1,3.0]|           [0.18633540372670...|\n",
      "+---+--------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "manhattanDistance = Normalizer().setP(1).setInputCol(\"features\")\n",
    "manhattanDistance.transform(scaleDF).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
