{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e585834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/26 07:10:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").\\\n",
    "                                     appName(\"spark_on_docker\").\\\n",
    "                                     getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f8026a",
   "metadata": {},
   "source": [
    "Formatting Models According to Your Use Case\n",
    "\n",
    "To preprocess data for Spark’s different advanced analytics tools, you must consider your end objective. The following list walks through the requirements for input data structure for each advanced analytics task in MLlib:\n",
    "\n",
    "    - In the case of most classification and regression algorithms, you want to get your data into a column of type Double to represent the label and a column of type Vector (either dense or sparse) to represent the features.\n",
    "\n",
    "    - In the case of recommendation, you want to get your data into a column of users, a column of items (say movies or books), and a column of ratings.\n",
    "\n",
    "    - In the case of unsupervised learning, a column of type Vector (either dense or sparse) is needed to represent the features.\n",
    "\n",
    "    - In the case of graph analytics, you will want a DataFrame of vertices and a DataFrame of edges.\n",
    "\n",
    "\n",
    "The best way to get your data in these formats is through transformers. Transformers are functions that accept a DataFrame as an argument and return a new DataFrame as a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11bbaf",
   "metadata": {},
   "source": [
    "Before we proceed, we’re going to read in several different sample datasets, each of which has\n",
    "different properties we will manipulate in this chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bc5a73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "sales = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\")\\\n",
    "    .coalesce(5)\\\n",
    "    .where(\"Description IS NOT NULL\")\n",
    "fakeIntDF = spark.read.parquet(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/simple-ml-integers\")\n",
    "simpleDF = spark.read.json(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/simple-ml\")\n",
    "scaleDF = spark.read.parquet(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/simple-ml-scaling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "167446dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.cache()\n",
    "sales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79e332",
   "metadata": {},
   "source": [
    "Transformers\n",
    "\n",
    "Transformers are functions that convert raw data in some way. This might be to create a new interaction variable (from two other variables), to normalize a column, or to simply turn it into a Double to be input into a model. Transformers are primarily used in preprocessing or feature generation.\n",
    "\n",
    "Spark’s transformer only includes a transform method. This is because it will not change based on the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cae211",
   "metadata": {},
   "source": [
    "Estimators for Preprocessing\n",
    "\n",
    "An estimator is necessary when a transformation you would like to perform must be initialized with data or information about the input column (often derived by doing a pass over the input column itself). \n",
    "\n",
    "For example, if you wanted to scale the values in our column to have mean zero and unit variance, you would need to perform a pass over the entire data in order to calculate the values you would use to normalize the data to mean zero and unit variance. In effect, an estimator can be a transformer configured according to your particular input data. In simplest terms, you can either blindly apply a transformation (a “regular” transformer type) or perform a transformation based on your data (an estimator type).\n",
    "\n",
    "An example of this type of estimator is the StandardScaler, which scales your input column according to the range of values in that column to have a zero mean and a variance of 1 in each dimension. For that reason it must first perform a pass over the data to create the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02ab39",
   "metadata": {},
   "source": [
    "High-Level Transformers\n",
    "\n",
    "High-level transformers allow you to concisely specify a number of transformations in one. These operate at a “high level”, and allow you to avoid doing data manipulations or transformations one by one. \n",
    "\n",
    "In general, you should try to use the highest level transformers you can, in order to minimize the risk of error and help you focus on the business problem instead of the smaller details of implementation. While this is not always possible, it’s a good objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887a3d4",
   "metadata": {},
   "source": [
    "RFormula\n",
    "\n",
    "The RFormula is the easiest transfomer to use when you have “conventionally” formatted data. \n",
    "\n",
    "Spark borrows this transformer from the R language to make it simple to declaratively specify a set of transformations for your data.\n",
    "\n",
    "The RFormula will automatically handle categorical inputs (specified as strings) by performing something called one-hot encoding. \n",
    "\n",
    "With the RFormula, numeric columns will be cast to Double but will not be one-hot encoded. If the label column is of type String, it will be first transformed to Double with StringIndexer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea63d20",
   "metadata": {},
   "source": [
    "The RFormula allows you to specify your transformations in declarative syntax. It is simple to use once you understand the syntax. Currently, RFormula supports a limited subset of the R operators that in practice work quite well for simple transformations. The basic operators are:\n",
    "    \n",
    "    ~\n",
    "    Separate target and terms\n",
    "\n",
    "    +\n",
    "    Concatenate terms; “+ 0” means removing the intercept (this means the y-intercept of the line\n",
    "    that we will fit will be 0)\n",
    "    \n",
    "    -\n",
    "    Remove a term; “- 1” means removing intercept (this means the y-intercept of the line that\n",
    "    we will fit will be 0)\n",
    "    \n",
    "    :\n",
    "    Interaction (multiplication for numeric values, or binarized categorical values)\n",
    "    \n",
    "    .\n",
    "    All columns except the target/dependent variabl\n",
    "\n",
    "RFormula also uses default columns of label and features to label, you guessed it, the label and the set of features that it outputs (for supervised machine learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05844885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
      "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\n",
    "supervised.fit(simpleDF).transform(simpleDF).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf4829",
   "metadata": {},
   "source": [
    "SQL Transformers\n",
    "\n",
    "A SQLTransformer allows you to leverage Spark’s vast library of SQL-related manipulations just as you would a MLlib transformation. \n",
    "\n",
    "Any SELECT statement you can use in SQL is a valid transformation. The only thing you need to change is that instead of using the table name, you should just use the keyword THIS. \n",
    "\n",
    "You might want to use SQLTransformer if you want to formally codify some DataFrame manipulation as a preprocessing step, or try different SQL expressions for features during hyperparameter tuning. Also note that the output of this transformation will be appended as a column to the output DataFrame.\n",
    "\n",
    "You might want to use an SQLTransformer in order to represent all of your manipulations on the very rawest form of your data so you can version different variations of manipulations as transformers. This gives you the benefit of building and testing varying pipelines, all by simply swapping out transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2a4f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|         1721|     119|   18180.0|\n",
      "|         1070|     107|   12782.0|\n",
      "|          701|      59|   17402.0|\n",
      "|          478|      35|   16642.0|\n",
      "|          477|      28|   16811.0|\n",
      "|          986|      71|   15053.0|\n",
      "|         1419|      50|   12913.0|\n",
      "|          445|      43|   12628.0|\n",
      "|         4505|     183|   14401.0|\n",
      "|          271|      20|   16851.0|\n",
      "+-------------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "basicTransformation = SQLTransformer()\\\n",
    "    .setStatement(\"\"\"\n",
    "                    SELECT sum(Quantity), count(*), CustomerID\n",
    "                    FROM __THIS__\n",
    "                    GROUP BY CustomerID\n",
    "    \"\"\")\n",
    "\n",
    "basicTransformation.transform(sales).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b708e",
   "metadata": {},
   "source": [
    "VectorAssembler\n",
    "\n",
    "The VectorAssembler is a tool you’ll use in nearly every single pipeline you generate. It helps concatenate all your features into one big vector you can then pass into an estimator. \n",
    "\n",
    "It’s used typically in the last step of a machine learning pipeline and takes as input a number of columns of Boolean, Double, or Vector. This is particularly helpful if you’re going to perform a number of manipulations using a variety of transformers and need to gather all of those results together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7510d2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_0ee9ec6ce765__output|\n",
      "+----+----+----+------------------------------------+\n",
      "|   1|   2|   3|                       [1.0,2.0,3.0]|\n",
      "|   4|   5|   6|                       [4.0,5.0,6.0]|\n",
      "|   7|   8|   9|                       [7.0,8.0,9.0]|\n",
      "+----+----+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d3141c",
   "metadata": {},
   "source": [
    "Working with Continuous Features\n",
    "\n",
    "Continuous features are just values on the number line, from positive infinity to negative infinity.\n",
    "\n",
    "There are two common transformers for continuous features. First, you can convert continuous features into categorical features via a process called bucketing, or you can scale and normalize your features according to several different requirements. These transformers will only work on Double types, so make sure you’ve turned any other numerical values to Double:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ef5510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78d8e3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  id|\n",
      "+----+\n",
      "| 0.0|\n",
      "| 1.0|\n",
      "| 2.0|\n",
      "| 3.0|\n",
      "| 4.0|\n",
      "| 5.0|\n",
      "| 6.0|\n",
      "| 7.0|\n",
      "| 8.0|\n",
      "| 9.0|\n",
      "|10.0|\n",
      "|11.0|\n",
      "|12.0|\n",
      "|13.0|\n",
      "|14.0|\n",
      "|15.0|\n",
      "|16.0|\n",
      "|17.0|\n",
      "|18.0|\n",
      "|19.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
