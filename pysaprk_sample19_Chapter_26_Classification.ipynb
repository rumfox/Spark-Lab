{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f26e6e6",
   "metadata": {},
   "source": [
    "Classification\n",
    "\n",
    "Classification is the task of predicting a label, category, class, or discrete variable given some input features. The key difference from other ML tasks, such as regression, is that the output label has a finite set of possible values (e.g., three classes).\n",
    "\n",
    "Types of Classification\n",
    "\n",
    "Binary Classification\n",
    "\n",
    "    The simplest example of classification is binary classification, where there are only two labels you can predict. One example is fraud analytics, where a given transaction can be classified as fraudulent or not; or email spam, where a given email can be classified as spam or not spam.\n",
    "\n",
    "Multiclass Classification\n",
    "\n",
    "    Beyond binary classification lies multiclass classification, where one label is chosen from more than two distinct possible labels. A typical example is Facebook predicting the people in a given photo or a meterologist predicting the weather (rainy, sunny, cloudy, etc.). Note how there is always a finite set of classes to predict; it’s never unbounded. This is also called multinomial classification.\n",
    "\n",
    "Multilabel Classification\n",
    "\n",
    "    Finally, there is multilabel classification, where a given input can produce multiple labels. For example, you might want to predict a book’s genre based on the text of the book itself. While this could be multiclass, it’s probably better suited for multilabel because a book may fall into multiple genres. Another example of multilabel classification is identifying the number of objects that appear in an image. Note that in this example, the number of output predictions is not necessarily fixed, and could vary from image to image.\n",
    "\n",
    "Classification Models in MLlib\n",
    "    \n",
    "    Spark has several models available for performing binary and multiclass classification out of the box. \n",
    "    The following models are available for classification in Spark:\n",
    "\n",
    "    Logistic regression\n",
    "    Decision trees\n",
    "    Random forests\n",
    "    Gradient-boosted trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e585834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").\\\n",
    "                                     appName(\"spark_on_docker\").\\\n",
    "                                     getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sales = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\")\\\n",
    "    .coalesce(5)\\\n",
    "    .where(\"Description IS NOT NULL\")\n",
    "'''\n",
    "\n",
    "bInput = spark.read.format(\"parquet\")\\\n",
    "    .load(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/binary-classification\")\\\n",
    "    .selectExpr(\"features\", \"cast(label as double) as label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "bInput.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bdabf6",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "\n",
    "Logistic regression is one of the most popular methods of classification. \n",
    "\n",
    "It is a linear method that combines each of the individual inputs (or features) with specific weights (these weights are generated during the training process) that are then combined to get a probability of belonging to a particular class. These weights are helpful because they are good representations of feature importance; if you have a large weight, you can assume that variations in that feature have a significant effect on the outcome (assuming you performed normalization). A smaller weight means the feature is less likely to be important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80991fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "print (lr.explainParams()) # see all parameters\n",
    "\n",
    "lrModel = lr.fit(bInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc57976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lrModel.coefficients)\n",
    "print (lrModel.intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa435d",
   "metadata": {},
   "source": [
    "Model Summary\n",
    "\n",
    "Logistic regression provides a model summary that gives you information about the final, trained model. \n",
    "\n",
    "Using the binary summary, we can get all sorts of information about the model itself including the area under the ROC curve, the f measure by threshold, the precision, the recall, the recall by thresholds, and the ROC curve. Note that for the area under the curve, instance weighting is not taken into account, so if you wanted to see how you performed on the values you weighed more highly, you’d have to do that manually. This will probably change in future Spark versions. You can see the summary using the following APIs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = lrModel.summary\n",
    "print (summary.areaUnderROC)\n",
    "\n",
    "summary.roc.show()\n",
    "summary.pr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3a4700",
   "metadata": {},
   "source": [
    "Decision Trees\n",
    "\n",
    "Decision trees are one of the more friendly and interpretable models for performing classification because they’re similar to simple decision models that humans use quite often. For example, if you have to predict whether or not someone will eat ice cream when offered, a good feature might be whether or not that individual likes ice cream.\n",
    "\n",
    "In pseudocode, if person.likes(“ice_cream”), they will eat ice cream; otherwise, they won’t eat ice cream. A decision tree creates this type of structure with all the inputs and follows a set of branches when it comes time to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "print (dt.explainParams()) \n",
    "print('-'*50)\n",
    "dtModel = dt.fit(bInput)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
