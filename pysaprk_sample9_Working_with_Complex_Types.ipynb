{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4e585834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").\\\n",
    "                                     appName(\"spark_on_docker\").\\\n",
    "                                     getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "04d29bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"work/TheDefinitiveGuide/Spark-The-Definitive-Guide/data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4dd58c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+--------------------+----------+\n",
      "|coalesce(Description, CustomerId)|         Description|CustomerId|\n",
      "+---------------------------------+--------------------+----------+\n",
      "|             WHITE HANGING HEA...|WHITE HANGING HEA...|   17850.0|\n",
      "|              WHITE METAL LANTERN| WHITE METAL LANTERN|   17850.0|\n",
      "|             CREAM CUPID HEART...|CREAM CUPID HEART...|   17850.0|\n",
      "|             KNITTED UNION FLA...|KNITTED UNION FLA...|   17850.0|\n",
      "|             RED WOOLLY HOTTIE...|RED WOOLLY HOTTIE...|   17850.0|\n",
      "|             SET 7 BABUSHKA NE...|SET 7 BABUSHKA NE...|   17850.0|\n",
      "|             GLASS STAR FROSTE...|GLASS STAR FROSTE...|   17850.0|\n",
      "|             HAND WARMER UNION...|HAND WARMER UNION...|   17850.0|\n",
      "|             HAND WARMER RED P...|HAND WARMER RED P...|   17850.0|\n",
      "|             ASSORTED COLOUR B...|ASSORTED COLOUR B...|   13047.0|\n",
      "|             POPPY'S PLAYHOUSE...|POPPY'S PLAYHOUSE...|   13047.0|\n",
      "|             POPPY'S PLAYHOUSE...|POPPY'S PLAYHOUSE...|   13047.0|\n",
      "|             FELTCRAFT PRINCES...|FELTCRAFT PRINCES...|   13047.0|\n",
      "|             IVORY KNITTED MUG...|IVORY KNITTED MUG...|   13047.0|\n",
      "|             BOX OF 6 ASSORTED...|BOX OF 6 ASSORTED...|   13047.0|\n",
      "|             BOX OF VINTAGE JI...|BOX OF VINTAGE JI...|   13047.0|\n",
      "|             BOX OF VINTAGE AL...|BOX OF VINTAGE AL...|   13047.0|\n",
      "|             HOME BUILDING BLO...|HOME BUILDING BLO...|   13047.0|\n",
      "|             LOVE BUILDING BLO...|LOVE BUILDING BLO...|   13047.0|\n",
      "|             RECIPE BOX WITH M...|RECIPE BOX WITH M...|   13047.0|\n",
      "+---------------------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\")), \"Description\", \"CustomerId\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ccdb1ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------------------+-----------------------+----------------------------------------+\n",
      "|ifnull(NULL, return_value)|nullif(value, value)|nvl(NULL, return_value)|nvl2(not_null, return_value, else_value)|\n",
      "+--------------------------+--------------------+-----------------------+----------------------------------------+\n",
      "|              return_value|                null|           return_value|                            return_value|\n",
      "+--------------------------+--------------------+-----------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT \\\n",
    "ifnull(null, 'return_value'),\\\n",
    "nullif('value', 'value'),\\\n",
    "nvl(null, 'return_value'),\\\n",
    "nvl2('not_null', 'return_value', 'else_value')\\\n",
    "FROM dfTable LIMIT 1\\\n",
    "\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7ea63",
   "metadata": {},
   "source": [
    "The simplest function is drop, which removes rows that contain nulls. \n",
    "The default is to drop any row in which any value is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7d5a21e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b3c3c",
   "metadata": {},
   "source": [
    "Specifying \"any\" as an argument drops a row if any of the values are null. \n",
    "Using “all” drops the row only if all values are null or NaN for that row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "47bef2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f8bc2",
   "metadata": {},
   "source": [
    "We can also apply this to certain sets of columns by passing in an array of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d5421299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc9f02",
   "metadata": {},
   "source": [
    "We can also do this with with a Scala Map, where the key is the column name and the value is the\n",
    "value we would like to use to fill null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "99b62405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_cols_vals = {\"StockCode\": 5, \"Description\" : \"No Value\"}\n",
    "df.na.fill(fill_cols_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547b2e8",
   "metadata": {},
   "source": [
    "replace\n",
    "\n",
    "Iqe case is to replace all values in a certain column according to their current value. The only requirement is that this value be the same type as the original value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eb7ab22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
